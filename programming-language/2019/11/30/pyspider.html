<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>pyspider 框架 | Balbo’s Blog</title>
<meta name="generator" content="Jekyll v4.3.2">
<meta property="og:title" content="pyspider 框架">
<meta name="author" content="Balbo Cheng">
<meta property="og:locale" content="en_US">
<meta name="description" content="介绍">
<meta property="og:description" content="介绍">
<link rel="canonical" href="https://balbocheng.com/jekyll-theme-yat/programming-language/2019/11/30/pyspider.html">
<meta property="og:url" content="https://balbocheng.com/jekyll-theme-yat/programming-language/2019/11/30/pyspider.html">
<meta property="og:site_name" content="Balbo’s Blog">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2019-11-30T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="pyspider 框架">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Balbo Cheng"},"dateModified":"2019-11-30T00:00:00+00:00","datePublished":"2019-11-30T00:00:00+00:00","description":"介绍","headline":"pyspider 框架","mainEntityOfPage":{"@type":"WebPage","@id":"https://balbocheng.com/jekyll-theme-yat/programming-language/2019/11/30/pyspider.html"},"url":"https://balbocheng.com/jekyll-theme-yat/programming-language/2019/11/30/pyspider.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="shortcut icon" href="assets/images/banners/favicon.ico">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/jekyll-theme-yat/assets/css/main.css">
  <script src="/jekyll-theme-yat/assets/js/main.js"></script><link type="application/atom+xml" rel="alternate" href="https://balbocheng.com/jekyll-theme-yat/feed.xml" title="Balbo's Blog">
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js"></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
</head>
<body>



























































































































<header class="site-header " role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/jekyll-theme-yat/">
  <img class="site-favicon" title="Balbo's Blog" src="assets/images/banners/favicon.ico" onerror="this.style.display='none'">
  Balbo's Blog
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">
<a class="page-link" href="/jekyll-theme-yat/">HOME</a><a class="page-link" href="/jekyll-theme-yat/archives.html">ARCHIVES</a><a class="page-link" href="/jekyll-theme-yat/categories.html">CATEGORIES</a><a class="page-link" href="/jekyll-theme-yat/tags.html">TAGS</a>









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'en',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="//translate.google.com/translate_a/element.js?cb=googleTranslateElementInit"></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>
















































































































































<script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('auto' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">pyspider 框架</h1>
  <h2 class="post-subtitle"></h2>

  <p class="post-meta">
    <time class="dt-published" datetime="2019-11-30T00:00:00+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Nov 30, 2019
    </time>

    
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 15 mins</span>
  </p>
<div class="post-tags">
<a class="post-tag" href="/jekyll-theme-yat/tags.html#pythin">#pythin</a><a class="post-tag" href="/jekyll-theme-yat/tags.html#reptile">#reptile</a>
</div></header>
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="介绍">介绍</h2>

<h3 id="pyspider-基本功能">pyspider 基本功能</h3>

<ul>
  <li>提供方便易用的 WebUI系统，可视化地编写和调试爬虫</li>
  <li>提供爬取进度监控、爬取结果查看、爬虫项目管理等功能</li>
  <li>支持多种候断数据库、如 MySQL、MongoDB、Redis、SQLite、Elasticsearch、PostgreSQL</li>
  <li>支持多种消息队列，如 RabbitMQ、Beanstalk、Redis、Kombu</li>
  <li>提供优先级控制、失败重试、定时抓取等功能</li>
  <li>对接了 PhantomJS，可以抓取 JavaScript 渲染的页面</li>
  <li>支持单机和分别是部署，支持 Docker 部署</li>
</ul>

<h3 id="与-scrapy-的比较">与 Scrapy 的比较</h3>

<ol>
  <li>pyspider 提供 WebUI,Scrapy它采用的是代码和命令行操作，但可以通过对接 Portia 现可视化配置</li>
  <li>pyspider 支持 PhantomJS来进行 JavaScript 谊染页面的采集 Scrapy 可以对接 Sc rapy-Splash组件，这需要额外配置</li>
  <li>pyspider 中内置pyquery 作为选择器而Scrapy 接了XPath 对接css选择器和正则匹配</li>
  <li>pyspider的可扩展程度不高，Scrapy可以通过对接其他的模块实现强大的功能，模块之间的耦合度低</li>
</ol>

<h3 id="总结">总结</h3>

<p>所以如果要快速实现一个页面的抓取，推荐使用 pyspider 开发更加便捷,如果要应对反爬程度很强、超大规模的抓取，推荐使用 Scrapy</p>

<h2 id="基本用法">基本用法</h2>

<h3 id="启动-pyspider">启动 pyspider</h3>

<p>在 CMD 下输出</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider all
</code></pre></div></div>

<p>这样可以启动 pyspider 的所有组件，包括 PhantomJS、ResultWorker、Processer、Fetcher、Scheduler、WebUI，这些都是 pyspider 运行必备的组件。最后一行输出提示 WebUI 运行在 5000 端口上。可以打开浏览器，输入链接 http://localhost:5000 ，这时我们会看到页面</p>

<h3 id="创建项目">创建项目</h3>

<p>新建一个项目，点击右边的 Create 按钮，在弹出的浮窗里输入项目的名称和爬取的链接，再点击 Create 按钮，这样就成功创建了一个项目</p>

<p>接下来会看到 pyspider 的项目编辑和调试页面</p>

<p>左侧就是代码的调试页面，点击左侧右上角的 run 单步调试爬虫程序，在左侧下半部分可以预览当前的爬取页面。右侧是代码编辑页面，我们可以直接编辑代码和保存代码，不需要借助于 IDE。</p>

<p>注意右侧，pyspider 已经帮我们生成了一段代码，代码如下所示：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pyspider.libs.base_handler</span> <span class="kn">import</span> <span class="o">*</span>

<span class="k">class</span> <span class="nc">Handler</span><span class="p">(</span><span class="n">BaseHandler</span><span class="p">):</span>
    <span class="n">crawl_config</span> <span class="o">=</span> <span class="p">{</span> <span class="p">}</span>

    <span class="nd">@every</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">24</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://travel.qunar.com/travelbook/list.htm</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">index_page</span><span class="p">)</span>

    <span class="nd">@config</span><span class="p">(</span><span class="n">age</span><span class="o">=</span><span class="mi">10</span> <span class="o">*</span> <span class="mi">24</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">index_page</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">a[href^=</span><span class="sh">"</span><span class="s">http</span><span class="sh">"</span><span class="s">]</span><span class="sh">'</span><span class="p">).</span><span class="nf">items</span><span class="p">():</span>
            <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="n">each</span><span class="p">.</span><span class="n">attr</span><span class="p">.</span><span class="n">href</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">detail_page</span><span class="p">)</span>

    <span class="nd">@config</span><span class="p">(</span><span class="n">priority</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">detail_page</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="sh">"</span><span class="s">url</span><span class="sh">"</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="n">url</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">).</span><span class="nf">text</span><span class="p">(),}</span>
</code></pre></div></div>

<p>这里的 Handler 就是 pyspider 爬虫的主类，我们可以在此处定义爬取、解析、存储的逻辑。整个爬虫的功能只需要一个 Handler 即可完成。</p>

<p>接下来我们可以看到一个 crawl_config 属性。我们可以将本项目的所有爬取配置统一定义到这里，如定义 Headers、设置代理等，配置之后全局生效。</p>

<p>然后，on_start() 方法是爬取入口，初始的爬取请求会在这里产生，该方法通过调用 crawl() 方法即可新建一个爬取请求，第一个参数是爬取的 URL，这里自动替换成我们所定义的 URL。crawl() 方法还有一个参数 callback，它指定了这个页面爬取成功后用哪个方法进行解析，代码中指定为 index_page() 方法，即如果这个 URL 对应的页面爬取成功了，那 Response 将交给 index_page() 方法解析。</p>

<p>index_page() 方法恰好接收这个 Response 参数，Response 对接了 pyquery。我们直接调用 doc() 方法传入相应的 CSS 选择器，就可以像 pyquery 一样解析此页面，代码中默认是 a[href^=”http”]，也就是说该方法解析了页面的所有链接，然后将链接遍历，再次调用了 crawl() 方法生成了新的爬取请求，同时再指定了 callback 为 detail_page，意思是说这些页面爬取成功了就调用 detail_page() 方法解析。这里，index_page() 实现了两个功能，一是将爬取的结果进行解析，二是生成新的爬取请求。</p>

<p>detail_page() 同样接收 Response 作为参数。detail_page() 抓取的就是详情页的信息，就不会生成新的请求，只对 Response 对象做解析，解析之后将结果以字典的形式返回。当然我们也可以进行后续处理，如将结果保存到数据库。</p>

<h3 id="爬取首页">爬取首页</h3>

<p>点击左栏右上角的 run 按钮，即可看到页面下方 follows 便会出现一个标注，其中包含数字 1，这代表有新的爬取请求产生</p>

<p>左栏左上角会出现当前 run 的配置文件，这里有一个 callback 为 on_start，这说明点击 run 之后实际是执行了 on_start() 方法。在 on_start() 方法中，我们利用 crawl() 方法生成一个爬取请求，那下方 follows 部分的数字 1 就代表了这一个爬取请求。</p>

<p>点击下方的 follows 按钮，即可看到生成的爬取请求的链接。每个链接的右侧还有一个箭头按钮</p>

<p>点击该箭头，我们就可以对此链接进行爬取，也就是爬取攻略的首页内容</p>

<p>上方的 callback 已经变成了 index_page，这就代表当前运行了 index_page() 方法。index_page() 接收到的 response 参数就是刚才生成的第一个爬取请求的 Response 对象。index_page() 方法通过调用 doc() 方法，传入提取所有 a 节点的 CSS 选择器，然后获取 a 节点的属性 href，这样实际上就是获取了第一个爬取页面中的所有链接。然后在 index_page() 方法里遍历了所有链接，同时调用 crawl() 方法，就把这一个个的链接构造成新的爬取请求了。所以最下方 follows 按钮部分有 217 的数字标记，这代表新生成了 217 个爬取请求，同时这些请求的 URL 都呈现在当前页面了。</p>

<p>再点击下方的 web 按钮，即可预览当前爬取结果的页面</p>

<p>当前看到的页面结果和浏览器看到的几乎是完全一致的，在这里我们可以方便地查看页面请求的结果。</p>

<p>点击 html 按钮即可查看当前页面的源代码</p>

<p>如果需要分析代码的结构，我们可以直接参考页面源码。</p>

<p>我们刚才在 index_page() 方法中提取了所有的链接并生成了新的爬取请求。但是很明显要爬取的肯定不是所有链接，只需要攻略详情的页面链接就够了，所以我们要修改一下当前 index_page() 里提取链接时的 CSS 选择器。</p>

<p>接下来需要另外一个工具。首先切换到 Web 页面，找到攻略的标题，点击下方的 enable css selector helper，点击标题。这时候我们看到标题外多了一个红框，上方出现了一个 CSS 选择器，这就是当前标题对应的 CSS 选择器</p>

<p>在右侧代码选中要更改的区域，点击左栏的右箭头，此时在上方出现的标题的 CSS 选择器就会被替换到右侧代码中</p>

<p>这样就成功完成了 CSS 选择器的替换，非常便捷。</p>

<p>重新点击左栏右上角的 run 按钮，即可重新执行 index_page() 方法。此时的 follows 就变成了 10 个，也就是说现在我们提取的只有当前页面的 10 个攻略</p>

<p>我们现在抓取的只是第一页的内容，还需要抓取后续页面，所以还需要一个爬取链接，即爬取下一页的攻略列表页面。我们再利用 crawl() 方法添加下一页的爬取请求，在 index_page() 方法里面添加如下代码，然后点击 save 保存：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">next</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">.next</span><span class="sh">'</span><span class="p">).</span><span class="n">attr</span><span class="p">.</span><span class="n">href</span>
<span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="nb">next</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">index_page</span><span class="p">)</span>
</code></pre></div></div>

<p>利用 CSS 选择器选中下一页的链接，获取它的 href 属性，也就获取了页面的 URL。然后将该 URL 传给 crawl() 方法，同时指定回调函数，注意这里回调函数仍然指定为 index_page() 方法，因为下一页的结构与此页相同。</p>

<p>重新点击 run 按钮，这时就可以看到 11 个爬取请求。follows 按钮上会显示 11，这就代表我们成功添加了下一页的爬取请求</p>

<p>现在，索引列表页的解析过程我们就完成了。</p>

<h3 id="爬取详情页">爬取详情页</h3>

<p>任意选取一个详情页进入，点击前 10 个爬取请求中的任意一个的右箭头，执行详情页的爬取</p>

<p>切换到 Web 页面预览效果，页面下拉之后，头图正文中的一些图片一直显示加载中</p>

<p>查看源代码，我们没有看到 img 节点</p>

<p>出现此现象的原因是 pyspider 默认发送 HTTP 请求，请求的 HTML 文档本身就不包含 img 节点。但是在浏览器中我们看到了图片，这是因为这张图片是后期经过 JavaScript 出现的。那么，我们该如何获取呢？</p>

<p>幸运的是，pyspider 内部对接了 PhantomJS，那么我们只需要修改一个参数即可。</p>

<p>我们将 index_page() 中生成抓取详情页的请求方法添加一个参数 fetch_type，改写的 index_page() 变为如下内容：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">index_page</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">each</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">li&gt; .tit &gt; a</span><span class="sh">'</span><span class="p">).</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="n">each</span><span class="p">.</span><span class="n">attr</span><span class="p">.</span><span class="n">href</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">detail_page</span><span class="p">,</span> <span class="n">fetch_type</span><span class="o">=</span><span class="sh">'</span><span class="s">js</span><span class="sh">'</span><span class="p">)</span>
    <span class="nb">next</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">.next</span><span class="sh">'</span><span class="p">).</span><span class="n">attr</span><span class="p">.</span><span class="n">href</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="nb">next</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">index_page</span><span class="p">)</span>
</code></pre></div></div>

<p>接下来，我们来试试它的抓取效果。</p>

<p>点击左栏上方的左箭头返回，重新调用 index_page() 方法生成新的爬取详情页的 Request</p>

<p>再点击新生成的详情页 Request 的爬取按钮，这时我们便可以看到页面变成了这样子</p>

<p>图片被成功渲染出来，这就是启用了 PhantomJS 渲染后的结果。只需要添加一个 fetch_type 参数即可，这非常方便。</p>

<p>最后就是将详情页中需要的信息提取出来，提取过程不再赘述。最终 detail_page() 方法改写如下所示：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">detail_page</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">url</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="n">url</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">title</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">#booktitle</span><span class="sh">'</span><span class="p">).</span><span class="nf">text</span><span class="p">(),</span>
        <span class="sh">'</span><span class="s">date</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">.when .data</span><span class="sh">'</span><span class="p">).</span><span class="nf">text</span><span class="p">(),</span>
        <span class="sh">'</span><span class="s">day</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">.howlong .data</span><span class="sh">'</span><span class="p">).</span><span class="nf">text</span><span class="p">(),</span>
        <span class="sh">'</span><span class="s">who</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">.who .data</span><span class="sh">'</span><span class="p">).</span><span class="nf">text</span><span class="p">(),</span>
        <span class="sh">'</span><span class="s">text</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">#b_panel_schedule</span><span class="sh">'</span><span class="p">).</span><span class="nf">text</span><span class="p">(),</span>
        <span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">:</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">.cover_img</span><span class="sh">'</span><span class="p">).</span><span class="n">attr</span><span class="p">.</span><span class="n">src</span>
    <span class="p">}</span>
</code></pre></div></div>

<p>我们分别提取了页面的链接、标题、出行日期、出行天数、人物、攻略正文、头图信息，将这些信息构造成一个字典。</p>

<p>重新运行，即可发现输出结果</p>

<p>左栏中输出了最终构造的字典信息，这就是一篇攻略的抓取结果。</p>

<h3 id="启动爬虫">启动爬虫</h3>

<p>返回爬虫的主页面，将爬虫的 status 设置成 DEBUG 或 RUNNING，点击右侧的 Run 按钮即可开始爬取</p>

<p>在最左侧我们可以定义项目的分组，以方便管理。rate/burst 代表当前的爬取速率，rate 代表 1 秒发出多少个请求，burst 相当于流量控制中的令牌桶算法的令牌数，rate 和 burst 设置的越大，爬取速率越快，当然速率需要考虑本机性能和爬取过快被封的问题。process 中的 5m、1h、1d 指的是最近 5 分、1 小时、1 天内的请求情况，all 代表所有的请求情况。请求由不同颜色表示，蓝色的代表等待被执行的请求，绿色的代表成功的请求，黄色的代表请求失败后等待重试的请求，红色的代表失败次数过多而被忽略的请求，这样可以直观知道爬取的进度和请求情况</p>

<p>点击 Active Tasks，即可查看最近请求的详细状况</p>

<p>点击 Results，即可查看所有的爬取结果</p>

<p>点击右上角的按钮，即可获取数据的 JSON、CSV 格式。</p>

<h2 id="用法详解">用法详解</h2>

<h3 id="命令行">命令行</h3>

<p>上面的实例通过如下命令启动 pyspider：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider all
</code></pre></div></div>

<p>命令行还有很多可配制参数，完整的命令行结构如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider <span class="o">[</span>OPTIONS] COMMAND <span class="o">[</span>ARGS]
</code></pre></div></div>

<p>其中，OPTIONS 为可选参数，它可以指定如下参数。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Options:
  <span class="nt">-c</span>, <span class="nt">--config</span> FILENAME    指定配置文件名称
  <span class="nt">--logging-config</span> TEXT    日志配置文件名称，默认: pyspider/pyspider/logging.conf
  <span class="nt">--debug</span>                  开启调试模式
  <span class="nt">--queue-maxsize</span> INTEGER  队列的最大长度
  <span class="nt">--taskdb</span> TEXT            taskdb 的数据库连接字符串，默认: sqlite
  <span class="nt">--projectdb</span> TEXT         projectdb 的数据库连接字符串，默认: sqlite
  <span class="nt">--resultdb</span> TEXT          resultdb 的数据库连接字符串，默认: sqlite
  <span class="nt">--message-queue</span> TEXT     消息队列连接字符串，默认: multiprocessing.Queue
  <span class="nt">--phantomjs-proxy</span> TEXT   PhantomJS 使用的代理，ip:port 的形式
  <span class="nt">--data-path</span> TEXT         数据库存放的路径
  <span class="nt">--version</span>                pyspider 的版本
  <span class="nt">--help</span>                   显示帮助信息
</code></pre></div></div>

<p>例如，-c 可以指定配置文件的名称，这是一个常用的配置，配置文件的样例结构如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>
  <span class="s2">"taskdb"</span>: <span class="s2">"mysql+taskdb://username:password@host:port/taskdb"</span>,
  <span class="s2">"projectdb"</span>: <span class="s2">"mysql+projectdb://username:password@host:port/projectdb"</span>,
  <span class="s2">"resultdb"</span>: <span class="s2">"mysql+resultdb://username:password@host:port/resultdb"</span>,
  <span class="s2">"message_queue"</span>: <span class="s2">"amqp://username:password@host:port/%2F"</span>,
  <span class="s2">"webui"</span>: <span class="o">{</span>
    <span class="s2">"username"</span>: <span class="s2">"some_name"</span>,
    <span class="s2">"password"</span>: <span class="s2">"some_passwd"</span>,
    <span class="s2">"need-auth"</span>: <span class="nb">true</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>如果要配置 pyspider WebUI 的访问认证，可以新建一个 pyspider.json，内容如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>
  <span class="s2">"webui"</span>: <span class="o">{</span>
    <span class="s2">"username"</span>: <span class="s2">"root"</span>,
    <span class="s2">"password"</span>: <span class="s2">"123456"</span>,
    <span class="s2">"need-auth"</span>: <span class="nb">true</span>
  <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>这样我们通过在启动时指定配置文件来配置 pyspider WebUI 的访问认证，用户名为 root，密码为 123456，命令如下所示：</p>

<p>pyspider -c pyspider.json all</p>

<p>运行之后打开：http://localhost:5000/</p>

<p>也可以单独运行 pyspider 的某一个组件。</p>

<p>运行 Scheduler 的命令如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider scheduler <span class="o">[</span>OPTIONS]
</code></pre></div></div>

<p>运行时也可以指定各种配置，参数如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Options:
  <span class="nt">--xmlrpc</span> /--no-xmlrpc
  <span class="nt">--xmlrpc-host</span> TEXT
  <span class="nt">--xmlrpc-port</span> INTEGER
  <span class="nt">--inqueue-limit</span> INTEGER  任务队列的最大长度，如果满了则新的任务会被忽略
  <span class="nt">--delete-time</span> INTEGER    设置为 delete 标记之前的删除时间
  <span class="nt">--active-tasks</span> INTEGER   当前活跃任务数量配置
  <span class="nt">--loop-limit</span> INTEGER     单轮最多调度的任务数量
  <span class="nt">--scheduler-cls</span> TEXT     Scheduler 使用的类
  <span class="nt">--help</span>                   显示帮助信息
</code></pre></div></div>

<p>运行 Fetcher 的命令如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider fetcher <span class="o">[</span>OPTIONS]
</code></pre></div></div>

<p>参数配置如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Options:
  <span class="nt">--xmlrpc</span> /--no-xmlrpc
  <span class="nt">--xmlrpc-host</span> TEXT
  <span class="nt">--xmlrpc-port</span> INTEGER
  <span class="nt">--poolsize</span> INTEGER      同时请求的个数
  <span class="nt">--proxy</span> TEXT            使用的代理
  <span class="nt">--user-agent</span> TEXT       使用的 User-Agent
  <span class="nt">--timeout</span> TEXT          超时时间
  <span class="nt">--fetcher-cls</span> TEXT      Fetcher 使用的类
  <span class="nt">--help</span>                  显示帮助信息
</code></pre></div></div>

<p>运行 Processer 的命令如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider processor <span class="o">[</span>OPTIONS]
</code></pre></div></div>

<p>参数配置如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Options:
  <span class="nt">--processor-cls</span> TEXT  Processor 使用的类
  <span class="nt">--help</span>                显示帮助信息
</code></pre></div></div>

<p>运行 WebUI 的命令如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider webui <span class="o">[</span>OPTIONS]
</code></pre></div></div>

<p>参数配置如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Options:
  <span class="nt">--host</span> TEXT            运行地址
  <span class="nt">--port</span> INTEGER         运行端口
  <span class="nt">--cdn</span> TEXT             JS 和 CSS 的 CDN 服务器
  <span class="nt">--scheduler-rpc</span> TEXT   Scheduler 的 xmlrpc 路径
  <span class="nt">--fetcher-rpc</span> TEXT     Fetcher 的 xmlrpc 路径
  <span class="nt">--max-rate</span> FLOAT       每个项目最大的 rate 值
  <span class="nt">--max-burst</span> FLOAT      每个项目最大的 burst 值
  <span class="nt">--username</span> TEXT        Auth 验证的用户名
  <span class="nt">--password</span> TEXT        Auth 验证的密码
  <span class="nt">--need-auth</span>            是否需要验证
  <span class="nt">--webui-instance</span> TEXT  运行时使用的 Flask 应用
  <span class="nt">--help</span>                 显示帮助信息
</code></pre></div></div>

<p>这里的配置和前面提到的配置文件参数是相同的。如果想要改变 WebUI 的端口为 5001，单独运行如下命令：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider webui <span class="nt">--port</span> 5001
</code></pre></div></div>

<p>或者可以将端口配置到 JSON 文件中，配置如下所示：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">{</span>
  <span class="s2">"webui"</span>: <span class="o">{</span><span class="s2">"port"</span>: 5001<span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div>

<p>使用如下命令启动同样可以达到相同的效果：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pyspider <span class="nt">-c</span> pyspider.json webui
</code></pre></div></div>

<p>这样就可以在 5001 端口上运行 WebUI 了。</p>

<h3 id="crawl-方法">crawl() 方法</h3>

<p>在前面的例子中，我们使用 crawl() 方法实现了新请求的生成，但是只指定了 URL 和 Callback。这里将详细介绍一下 crawl() 方法的参数配置。</p>

<ul>
  <li>
    <p>url</p>

    <p>url 是爬取时的 URL，可以定义为单个 URL 字符串，也可以定义成 URL 列表。</p>
  </li>
  <li>
    <p>callback</p>

    <p>callback 是回调函数，指定了该 URL 对应的响应内容用哪个方法来解析，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://scrapy.org/</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">index_page</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>这里指定了 callback 为 index_page，就代表爬取 http://scrapy.org/ 链接得到的响应会用 index_page() 方法来解析。</p>

    <p>index_page() 方法的第一个参数是响应对象，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">index_page</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div>    </div>

    <p>方法中的 response 参数就是请求上述 URL 得到的响应对象，我们可以直接在 index_page() 方法中实现页面的解析。</p>
  </li>
  <li>
    <p>age</p>

    <p>age 是任务的有效时间。如果某个任务在有效时间内且已经被执行，则它不会重复执行，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://www.example.org/</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span>
               <span class="n">age</span><span class="o">=</span><span class="mi">10</span><span class="o">*</span><span class="mi">24</span><span class="o">*</span><span class="mi">60</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>或者可以这样设置：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="nd">@config</span><span class="p">(</span><span class="n">age</span><span class="o">=</span><span class="mi">10</span> <span class="o">*</span> <span class="mi">24</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">callback</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div>    </div>

    <p>默认的有效时间为 10 天。</p>
  </li>
  <li>
    <p>priority</p>

    <p>priority 是爬取任务的优先级，其值默认是 0，priority 的数值越大，对应的请求会越优先被调度，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">index_page</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://www.example.org/page.html</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">index_page</span><span class="p">)</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://www.example.org/233.html</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">detail_page</span><span class="p">,</span>
               <span class="n">priority</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>第二个任务会优先调用，233.html 这个链接优先爬取。</p>
  </li>
  <li>
    <p>exetime</p>

    <p>exetime 参数可以设置定时任务，其值是时间戳，默认是 0，即代表立即执行，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">time</span>
<span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://www.example.org/</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span>
               <span class="n">exetime</span><span class="o">=</span><span class="n">time</span><span class="p">.</span><span class="nf">time</span><span class="p">()</span><span class="o">+</span><span class="mi">30</span><span class="o">*</span><span class="mi">60</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>这样该任务会在 30 分钟之后执行。</p>
  </li>
  <li>
    <p>retries</p>

    <p>retries 可以定义重试次数，其值默认是 3。</p>
  </li>
  <li>
    <p>itag</p>

    <p>itag 参数设置判定网页是否发生变化的节点值，在爬取时会判定次当前节点是否和上次爬取到的节点相同。如果节点相同，则证明页面没有更新，就不会重复爬取，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">index_page</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">response</span><span class="p">.</span><span class="nf">doc</span><span class="p">(</span><span class="sh">'</span><span class="s">.item</span><span class="sh">'</span><span class="p">).</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="n">item</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">).</span><span class="n">attr</span><span class="p">.</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">detail_page</span><span class="p">,</span>
                   <span class="n">itag</span><span class="o">=</span><span class="n">item</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="sh">'</span><span class="s">.update-time</span><span class="sh">'</span><span class="p">).</span><span class="nf">text</span><span class="p">())</span>
</code></pre></div>    </div>

    <p>在这里设置了更新时间这个节点的值为 itag，在下次爬取时就会首先检测这个值有没有发生变化，如果没有变化，则不再重复爬取，否则执行爬取。</p>
  </li>
  <li>
    <p>auto_recrawl</p>

    <p>当开启时，爬取任务在过期后会重新执行，循环时间即定义的 age 时间长度，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://www.example.org/</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span>
               <span class="n">age</span><span class="o">=</span><span class="mi">5</span><span class="o">*</span><span class="mi">60</span><span class="o">*</span><span class="mi">60</span><span class="p">,</span> <span class="n">auto_recrawl</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>这里定义了 age 有效期为 5 小时，设置了 auto_recrawl 为 True，这样任务就会每 5 小时执行一次。</p>
  </li>
  <li>
    <p>method</p>

    <p>method 是 HTTP 请求方式，它默认是 GET。如果想发起 POST 请求，可以将 method 设置为 POST。</p>
  </li>
  <li>
    <p>params</p>

    <p>我们可以方便地使用 params 来定义 GET 请求参数，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://httpbin.org/get</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span>
               <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">123</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">})</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://httpbin.org/get?a=123&b=c</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>这里两个爬取任务是等价的。</p>
  </li>
  <li>
    <p>data</p>

    <p>data 是 POST 表单数据。当请求方式为 POST 时，我们可以通过此参数传递表单数据，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://httpbin.org/post</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span>
               <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">POST</span><span class="sh">'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">a</span><span class="sh">'</span><span class="p">:</span> <span class="mi">123</span><span class="p">,</span> <span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">c</span><span class="sh">'</span><span class="p">})</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>files</p>

    <p>files 是上传的文件，需要指定文件名，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://httpbin.org/post</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span>
               <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">POST</span><span class="sh">'</span><span class="p">,</span> <span class="n">files</span><span class="o">=</span><span class="p">{</span><span class="n">field</span><span class="p">:</span> <span class="p">{</span><span class="n">filename</span><span class="p">:</span> <span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">}})</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>user_agent</p>

    <p>user_agent 是爬取使用的 User-Agent。</p>
  </li>
  <li>
    <p>headers</p>

    <p>headers 是爬取时使用的 Headers，即 Request Headers。</p>
  </li>
  <li>
    <p>cookies</p>

    <p>cookies 是爬取时使用的 Cookies，为字典格式。</p>
  </li>
  <li>
    <p>connect_timeout</p>

    <p>connect_timeout 是在初始化连接时的最长等待时间，它默认是 20 秒。</p>
  </li>
  <li>
    <p>timeout</p>

    <p>timeout 是抓取网页时的最长等待时间，它默认是 120 秒。</p>
  </li>
  <li>
    <p>allow_redirects</p>

    <p>allow_redirects 确定是否自动处理重定向，它默认是 True。</p>
  </li>
  <li>
    <p>validate_cert</p>

    <p>validate_cert 确定是否验证证书，此选项对 HTTPS 请求有效，默认是 True。</p>
  </li>
  <li>
    <p>proxy</p>

    <p>proxy 是爬取时使用的代理，它支持用户名密码的配置，格式为 username:password@hostname:port，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://httpbin.org/get</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span> <span class="n">proxy</span><span class="o">=</span><span class="sh">'</span><span class="s">127.0.0.1:9743</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>也可以设置 craw_config 来实现全局配置，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Handler</span><span class="p">(</span><span class="n">BaseHandler</span><span class="p">):</span>
    <span class="n">crawl_config</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">proxy</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">127.0.0.1:9743</span><span class="sh">'</span><span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>fetch_type</p>

    <p>fetch_type 开启 PhantomJS 渲染。如果遇到 JavaScript 渲染的页面，指定此字段即可实现 PhantomJS 的对接，pyspider 将会使用 PhantomJS 进行网页的抓取，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">https://www.taobao.com</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">index_page</span><span class="p">,</span> <span class="n">fetch_type</span><span class="o">=</span><span class="sh">'</span><span class="s">js</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>这样我们就可以实现淘宝页面的抓取了，得到的结果就是浏览器中看到的效果。</p>
  </li>
  <li>
    <p>js_script</p>

    <p>js_script 是页面加载完毕后执行的 JavaScript 脚本，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://www.example.org/</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span>
               <span class="n">fetch_type</span><span class="o">=</span><span class="sh">'</span><span class="s">js</span><span class="sh">'</span><span class="p">,</span> <span class="n">js_script</span><span class="o">=</span><span class="sh">'''</span><span class="s">
               function() {window.scrollTo(0,document.body.scrollHeight);
                   return 123;
               }
               </span><span class="sh">'''</span><span class="p">)</span>
</code></pre></div>    </div>

    <p>页面加载成功后将执行页面混动的 JavaScript 代码，页面会下拉到最底部。</p>
  </li>
  <li>
    <p>js_run_at</p>

    <p>js_run_at 代表 JavaScript 脚本运行的位置，是在页面节点开头还是结尾，默认是结尾，即 document-end。</p>
  </li>
  <li>
    <p>js_viewport_width/js_viewport_height</p>

    <p>js_viewport_width/js_viewport_height 是 JavaScript 渲染页面时的窗口大小。</p>
  </li>
  <li>
    <p>load_images</p>

    <p>load_images 在加载 JavaScript 页面时确定是否加载图片，它默认是否。</p>
  </li>
  <li>
    <p>save</p>

    <p>save 参数非常有用，可以在不同的方法之间传递参数，如下所示：</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://www.example.org/</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">callback</span><span class="p">,</span>
               <span class="n">save</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">page</span><span class="sh">'</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
  
<span class="k">def</span> <span class="nf">callback</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">response</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">response</span><span class="p">.</span><span class="n">save</span><span class="p">[</span><span class="sh">'</span><span class="s">page</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div>    </div>

    <p>这样，在 on_start() 方法中生成 Request 并传递额外的参数 page，在回调函数里可以通过 response 变量的 save 字段接收到这些参数值。</p>
  </li>
  <li>
    <p>cancel</p>

    <p>cancel 是取消任务，如果一个任务是 ACTIVE 状态的，则需要将 force_update 设置为 True。
force_update</p>

    <p>即使任务处于 ACTIVE 状态，那也会强制更新状态。</p>

    <p>以上便是 crawl() 方法的参数介绍，更加详细的描述可以参考： http://docs.pyspider.org/en/latest/apis/self.crawl/。</p>
  </li>
</ul>

<h3 id="任务区分">任务区分</h3>

<p>在 pyspider 判断两个任务是否是重复的是使用的是该任务对应的 URL 的 MD5 值作为任务的唯一 ID，如果 ID 相同，那么两个任务就会判定为相同，其中一个就不会爬取了。很多情况下请求的链接可能是同一个，但是 POST 的参数不同。这时可以重写 task_id() 方法，改变这个 ID 的计算方式来实现不同任务的区分，如下所示：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">json</span>
<span class="kn">from</span> <span class="n">pyspider.libs.utils</span> <span class="kn">import</span> <span class="n">md5string</span>
<span class="k">def</span> <span class="nf">get_taskid</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">task</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">md5string</span><span class="p">(</span><span class="n">task</span><span class="p">[</span><span class="sh">'</span><span class="s">url</span><span class="sh">'</span><span class="p">]</span><span class="o">+</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">(</span><span class="n">task</span><span class="p">[</span><span class="sh">'</span><span class="s">fetch</span><span class="sh">'</span><span class="p">].</span><span class="nf">get</span><span class="p">(</span><span class="sh">'</span><span class="s">data</span><span class="sh">'</span><span class="p">,</span> <span class="sh">''</span><span class="p">)))</span>
</code></pre></div></div>

<p>这里重写了 get_taskid() 方法，利用 URL 和 POST 的参数来生成 ID。这样一来，即使 URL 相同，但是 POST 的参数不同，两个任务的 ID 就不同，它们就不会被识别成重复任务。</p>

<h3 id="全局配置">全局配置</h3>

<p>pyspider 可以使用 crawl_config 来指定全局的配置，配置中的参数会和 crawl() 方法创建任务时的参数合并。如要全局配置一个 Headers，可以定义如下代码：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Handler</span><span class="p">(</span><span class="n">BaseHandler</span><span class="p">):</span>
    <span class="n">crawl_config</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">headers</span><span class="sh">'</span><span class="p">:</span> <span class="p">{</span><span class="sh">'</span><span class="s">User-Agent</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">GoogleBot</span><span class="sh">'</span><span class="p">,}</span>
    <span class="p">}</span>
</code></pre></div></div>

<h3 id="定时爬取">定时爬取</h3>

<p>我们可以通过 every 属性来设置爬取的时间间隔，如下所示：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@every</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">24</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">url</span> <span class="ow">in</span> <span class="n">urllist</span><span class="p">:</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">index_page</span><span class="p">)</span>
</code></pre></div></div>

<p>这里设置了每天执行一次爬取。</p>

<p>在上文中我们提到了任务的有效时间，在有效时间内爬取不会重复。所以要把有效时间设置得比重复时间更短，这样才可以实现定时爬取。</p>

<p>例如，下面的代码就无法做到每天爬取：</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nd">@every</span><span class="p">(</span><span class="n">minutes</span><span class="o">=</span><span class="mi">24</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">on_start</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="n">self</span><span class="p">.</span><span class="nf">crawl</span><span class="p">(</span><span class="sh">'</span><span class="s">http://www.example.org/</span><span class="sh">'</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">index_page</span><span class="p">)</span>

<span class="nd">@config</span><span class="p">(</span><span class="n">age</span><span class="o">=</span><span class="mi">10</span> <span class="o">*</span> <span class="mi">24</span> <span class="o">*</span> <span class="mi">60</span> <span class="o">*</span> <span class="mi">60</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">index_page</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
    <span class="k">pass</span>
</code></pre></div></div>

<p>这里任务的过期时间为 10 天，而自动爬取的时间间隔为 1 天。当第二次尝试重新爬取的时候，pyspider 会监测到此任务尚未过期，便不会执行爬取，所以我们需要将 age 设置得小于定时时间。</p>

<h3 id="项目状态">项目状态</h3>

<p>每个项目都有 6 个状态，分别是 TODO、STOP、CHECKING、DEBUG、RUNNING、PAUSE。</p>

<ul>
  <li>TODO：它是项目刚刚被创建还未实现时的状态。</li>
  <li>STOP：如果想停止某项目的抓取，可以将项目的状态设置为 STOP。</li>
  <li>CHECKING：正在运行的项目被修改后就会变成 CHECKING 状态，项目在中途出错需要调整的时候会遇到这种情况。</li>
  <li>DEBUG/RUNNING：这两个状态对项目的运行没有影响，状态设置为任意一个，项目都可以运行，但是可以用二者来区分项目是否已经测试通过。</li>
  <li>PAUSE：当爬取过程中出现连续多次错误时，项目会自动设置为 PAUSE 状态，并等待一定时间后继续爬取。</li>
</ul>

<h3 id="爬取进度">爬取进度</h3>

<p>在抓取时，可以看到抓取的进度，progress 部分会显示 4 个进度条</p>

<p>progress 中的 5m、1h、1d 指的是最近 5 分、1 小时、1 天内的请求情况，all 代表所有的请求情况。</p>

<p>蓝色的请求代表等待被执行的任务，绿色的代表成功的任务，黄色的代表请求失败后等待重试的任务，红色的代表失败次数过多而被忽略的任务，从这里我们可以直观看到爬取的进度和请求情况。</p>

<h3 id="删除项目">删除项目</h3>

<p>pyspider 中没有直接删除项目的选项。如要删除任务，那么将项目的状态设置为 STOP，将分组的名称设置为 delete，等待 24 小时，则项目会自动删除。</p>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/jekyll-theme-yat/programming-language/2019/11/29/login.html" title="模拟登录">模拟登录</a><a class="next" href="/jekyll-theme-yat/software_designer/2021/03/14/data-structure.html" title="数据结构基础知识">数据结构基础知识</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li><a class="post-link" href="/jekyll-theme-yat/programming-language/2019/11/20/beautifulsoup.html" title="数据结构基础知识">Beautiful Soup</a></li>
<li><a class="post-link" href="/jekyll-theme-yat/programming-language/2019/10/31/xapx.html" title="数据结构基础知识">Python 运行问题</a></li>
<li><a class="post-link" href="/jekyll-theme-yat/programming-language/2022/03/23/target_detection.html" title="数据结构基础知识">目标检测</a></li>
<li><a class="post-link" href="/jekyll-theme-yat/programming-language/2019/11/17/xpath.html" title="数据结构基础知识">XPath</a></li>
</ul>
    </div>
<div class="post-comments">  <div id="disqus_thread"></div>
  <script>
    var disqus_config = function () {
      this.page.url = 'https://balbocheng.com/jekyll-theme-yat/programming-language/2019/11/30/pyspider.html';
      this.page.identifier = 'https://balbocheng.com/jekyll-theme-yat/programming-language/2019/11/30/pyspider.html';
    };

    (function() {
      var d = document, s = d.createElement('script');

      s.src = 'https://Balbo.disqus.com/embed.js';

      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a>
</noscript>
</div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --></section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/jekyll-theme-yat/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright © 2018-2023 Balbo Cheng</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/jekyll-theme-yat/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
</html>
