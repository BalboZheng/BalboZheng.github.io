---
layout:     post
title:      "K-近邻算法"
subtitle:   ""
date:       2022-03-05 11:18:15
author:     "Balbo"
header-img: "img/post-bg-2022.png"
catalog: true
tags:
    - python
---

## K-近邻算法

kNN是一种基本的分类和回归方法。kNN的输入是测试数据和训练样本数据集，输出是测试样本的类别。kNN没有显示的训练过程，在测试时，计算测试样本和所有训练样本的距离，根据最近的K个训练样本的类别，通过多数投票的方式进行预测。

![](https://img2018.cnblogs.com/blog/152332/201810/152332-20181008195559660-2025655795.png)

上图中，绿色圆要被决定赋予哪个类，是红色三角形还是蓝色四方形？如果K=3，由于红色三角形所占比例为2/3，绿色圆将被赋予红色三角形那个类，如果K=5，由于蓝色四方形比例为3/5，因此绿色圆被赋予蓝色四方形类

### 算法步骤

- 算距离： 给定测试对象，计算它与训练集中的每个对象的距离；
- 找邻居：圈定距离最近的k个训练对象，作为测试对象的近邻；
- 做分类：根据这k个近邻归属的主要类别，来对测试对象分类

### 类别法判定

- 投票法：少数服从多数，近邻中哪个类别的点最多就分为该类。
- 加权投票法：根据距离的远近，对邻近的投票进行加权，距离越近则权重越大（权重为距离平方的倒数）。

## k近邻算法api

```python
# n_neighbors 可选，查询默认使用的邻居数
sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)
```

### 案例

- 获取数据集
- 数据基本处理
- 特征工程
- 机器学习
- 模型评估

```python
from sklearn.neighbors import KNeighborsClassifier

# 构造数据集
x = [[0], [1], [2], [3]]
y = [0, 0, 1, 2]

# 机器学习
# 实例化API
estimator = KNeighborsClassifier(n_neighbors=2)
# 使用fit方法进行训练
estimator.fit(x, y)

estimator.predict([[1]])

```

## 距离度量

### 1. 欧式距离

通过距离平方值进行计算
$$
d_{euc}(x,y)=[\sum_{j=1}^d(x_j-y_j)^2]^{\frac{1}{2}}=[(x-y)(x-y)^T]^{\frac{1}{2}}
$$

### 2. 曼哈顿距离（City Block distance）

通过举例的绝对值进行计算
$$
d_{man}(x,y)=\sum_{j=1}^{d}|x_j-y_j|
$$

### 3. 切里雪夫距离

维度的最大值进行计算
$$
d_{qie}= max(|x_{1i}-x_{2i}|)
$$

> 缺点：将各个分量的量纲，也就是单位相同看待了；未考虑各个分量的发布（期望、方差等）可能不同

### 4. 标准化欧式距离

再计算过程中添加了标准差（**S 为标准差**），对量纲进行处理
$$
d_{euc}(x,y)=[\sum_{j=1}^d(\frac{x_j-y_j}{S_j})^2]^{\frac{1}{2}}
$$

### 5. 余弦距离

通过 cos 思想进行计算
$$
cos(\theta)=\frac{x_1x_2+y_1y_2}{\sqrt{x_1^2+y_1^2}\sqrt{x_2^2+y_2^2}}=\frac{a*b}{|a||b|}
$$

## k值选择

k值过小：容易受到异常点影响 (过拟合)

k值过大：收到样本均衡的影响（欠拟合）

### 误差

近似误差：可以理解为对现有训练集的训练误差。
估计误差：可以理解为对测试集的测试误差。

## kd 树

kd树（k-dimensional树的简称），是一种分割k维数据空间的[数据结构](https://so.csdn.net/so/search?q=数据结构&spm=1001.2101.3001.7020)。主要应用于多维空间关键数据的搜索（如：范围搜索和最近邻搜索）。

### 二维的节点怎么比较大小？

在BSTree中，节点分割的是一维数轴，那么在二维中，就应当是分割平面了，就像这样：

![](https://img-blog.csdn.net/20141125170640843)

黄色的点作为根节点，上面的点归左子树，下面的点归右子树，接下来再不断地划分，最后得到一棵树就是赫赫有名的BSPTree（binary space partitioning tree）. 分割的那条线叫做分割超平面（splitting hyperplane），在一维中是一个点，二维中是线，三维的是面。
KDTree就是超平面都垂直于轴的BSPTree。同样的数据集，用KDTree划分之后就是这样：

![](https://img-blog.csdn.net/20141126094947004)

黄色节点就是Root节点，下一层是红色，再下一层是绿色，再下一层是蓝色。为了更好的理解KDTree的分割，我们在图形中来形象地看一下搜索的过程，假设现在需要搜寻右下角的一个点，首先要做的就是比较这个点的x坐标和root点的x坐标值，由于x坐标值大于root节点的x坐标，所以只需要在右边搜寻，接下来，要比较该节点和右边红色节点y值得大小...后面依此类推。整个过程如下图：

![](https://img-blog.csdn.net/20141126095837750) ![](https://img-blog.csdn.net/20141126095909812) ![](https://img-blog.csdn.net/20141126095925156)

### 树的建立

先定义一下节点的数据结构。每个节点应当有下面几个域：

Node-data -  数据矢量， 数据集中某个数据点，是n维矢量（这里也就是k维）
Range  - 空间矢量， 该节点所代表的空间范围
split  - 整数， 垂直于分割超平面的方向轴序号
Left  - k-d树， 由位于该节点分割超平面左子空间内所有数据点所构成的k-d树
Right  - k-d树， 由位于该节点分割超平面右子空间内所有数据点所构成的k-d树
parent  - k-d树， 父节点

建立树最大的问题在于轴点（pivot）的选择，选择好轴点之后，树的建立就和BSTree差不多了。
建树必须遵循两个准则：
1.建立的树应当尽量平衡，树越平衡代表着分割得越平均，搜索的时间也就是越少。
2.最大化邻域搜索的剪枝机会

