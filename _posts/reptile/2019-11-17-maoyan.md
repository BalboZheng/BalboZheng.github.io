---
layout:     post
title:      "电影抓取"
subtitle:   "抓取猫眼电影排行"
date:       2019-11-17 14:28:25
author:     "Balbo"
header-img: "img/post-bg-2019.jpg"
tags:
    - reptile
---
## 抓取首页
```python
import requests


def get_one_page(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, '
                      'like Gecko) Version/5.1 Safari/534.50 '
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.text
    return None


def main():
    url = 'https://maoyan.com/board/4'
    html = get_one_page(url)
    print(html)


if __name__ == '__main__':
    main()
```
## 正则提取
回到网页，按F12在开发者模式下的NETWORK监听组件中查看到源代码

在源代码中可以看到电影的相关信息，这里用红色箭头从上至下分别标记出了电影的排名、图片、电影名、演员、上映时间以及评分所在的位置。接下来只需要为每一部分的信息写出对应的正则表达式即可获取到相应的信息。

先以电影排名为例，可以看到每一个电影部分的所有信息都包含在< dd>..<\/dd>节点中，而电影排名的信息是在class为board-index的i节点中，所以这里利用非贪婪的匹配原则提取该节点内的信息，正则表达式可写为：

> <dd>.*?"board-index.*?>(.*?)</i>

接下来获取电影图片。可以看到电影图片的信息在img data-src的a节点中，正则表达式可写为：

> <dd>.*?data-src="(.*?)".*?</a>

同样，电影名称的信息在name为标志位title=""的a节点中，正则表达式可写为：

> <dd>.*?class="name".*?title="(.*?)".*?</a>

按照以上方法，最后可以得到完整的正则表达式为：

><dd>.*?board-index.*?>(\d+?)</i>.*?data-src="(.*?)".*?</a>.*?class="name".*?title="(.*?)".*?</a>.*?class="star".+?(.*?)</p>.*?class="releasetime".+?(.*?)</p>.*?class="integer".+?(.*?)</i>.*?"fraction".+?(.*?)</i>.*?</dd>

接下来只需要构建一个解析页面的函数parse_one_page()，用来得到所需要的电影信息，代码实现如下：
```python
def parse_one_page(html):
    pattern = re.compile(
        r'<dd>.*?board-index.*?>(\d+?)</i>'
        r'.*?data-src="(.*?)".*?</a>'
        r'.*?class="name".*?title="(.*?)".*?</a>'
        r'.*?class="star".+?(.*?)</p>'
        r'.*?class="releasetime".+?(.*?)</p>'
        r'.*?class="integer".+?(.*?)</i>'
        r'.*?"fraction".+?(.*?)</i>.*?</dd>', re.S
    )
    re_lists = re.findall(pattern, html)
    for re_list in re_lists:
        yield {
            'index': re_list[0],
            'image': re_list[1],
            'title': re_list[2],
            'actor': re_list[3].strip()[3:],
            'time': re_list[4].strip()[5:],
            'score': re_list[5] + re_list[6]
        }
```

## 写入文件
对于抓取到的数据，这里构建一个write()函数，直接将数据写入文本文件中。通过JSON库里的dumps()方法实现字典的序列化，并指定ensure_ascii参数为False，这样可以保证输出结果是中文形式而不是Unicode编码。代码实现如下：

```python
def write_to_file(content):
    with open('result.txt', 'a', encoding='utf-8') as f:
        print(type(json.dumps(content)))
        f.write(json.dumps(content, ensure_ascii=False)+'\n')
```
## 分页爬取
因为我们需要爬取的是TOP100，所以还需要遍历一下，给这个连接传入 offset 参数，实现其他90不电影的爬取
```python
if __name__ == '__main__':
    for i in range(10):
        main(offset=i * 10)
```
```python
def main(offset):
    url = 'https://maoyan.com/board/4?offset=' + str(offset)
    html = get_one_page(url)
    for item in parse_one_page(html):
        write_to_file(item)
```

## 整合代码
```python
import json
import re

import requests


def get_one_page(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, '
                      'like Gecko) Version/5.1 Safari/534.50 '
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.text
    return None


def parse_one_page(html):
    pattern = re.compile(
        r'<dd>.*?board-index.*?>(\d+?)</i>'
        r'.*?data-src="(.*?)".*?</a>'
        r'.*?class="name".*?title="(.*?)".*?</a>'
        r'.*?class="star".+?(.*?)</p>'
        r'.*?class="releasetime".+?(.*?)</p>'
        r'.*?class="integer".+?(.*?)</i>'
        r'.*?"fraction".+?(.*?)</i>.*?</dd>', re.S
    )
    re_lists = re.findall(pattern, html)
    for re_list in re_lists:
        yield {
            'index': re_list[0],
            'image': re_list[1],
            'title': re_list[2],
            'actor': re_list[3].strip()[3:],
            'time': re_list[4].strip()[5:],
            'score': re_list[5] + re_list[6]
        }


def write_to_file(content):
    with open('result.txt', 'a', encoding='utf-8') as f:
        print(type(json.dumps(content)))
        f.write(json.dumps(content, ensure_ascii=False)+'\n')


def main():
    url = 'https://maoyan.com/board/4'
    html = get_one_page(url)
    for item in parse_one_page(html):
        write_to_file(item)


if __name__ == '__main__':
    main()
```
