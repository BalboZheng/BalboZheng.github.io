---
layout:     post
title:      "pyspider 框架"
subtitle:   "了解学习pyspider框架"
date:       2019-11-30 21:00:20
author:     "Balbo"
header-img: "img/post-bg-2019.jpg"
tags:
    - reptile
---
## 介绍
### pyspider 基本功能

- 提供方便易用的 WebUI系统，可视化地编写和调试爬虫
- 提供爬取进度监控、爬取结果查看、爬虫项目管理等功能
- 支持多种候断数据库、如 MySQL、MongoDB、Redis、SQLite、Elasticsearch、PostgreSQL
- 支持多种消息队列，如 RabbitMQ、Beanstalk、Redis、Kombu
- 提供优先级控制、失败重试、定时抓取等功能
- 对接了 PhantomJS，可以抓取 JavaScript 渲染的页面
- 支持单机和分别是部署，支持 Docker 部署

### 与 Scrapy 的比较

1. pyspider 提供 WebUI,Scrapy它采用的是代码和命令行操作，但可以通过对接 Portia 现可视化配置
2. pyspider 支持 PhantomJS来进行 JavaScript 谊染页面的采集 Scrapy 可以对接 Sc rapy-Splash组件，这需要额外配置
3. pyspider 中内置pyquery 作为选择器而Scrapy 接了XPath 对接css选择器和正则匹配
4. pyspider的可扩展程度不高，Scrapy可以通过对接其他的模块实现强大的功能，模块之间的耦合度低

### 总结

所以如果要快速实现一个页面的抓取，推荐使用 pyspider 开发更加便捷,如果要应对反爬程度很强、超大规模的抓取，推荐使用 Scrapy 

## 基本用法
### 启动 pyspider

在 CMD 下输出
```
pyspider all
```
这样可以启动 pyspider 的所有组件，包括 PhantomJS、ResultWorker、Processer、Fetcher、Scheduler、WebUI，这些都是 pyspider 运行必备的组件。最后一行输出提示 WebUI 运行在 5000 端口上。可以打开浏览器，输入链接 http://localhost:5000 ，这时我们会看到页面

### 创建项目

新建一个项目，点击右边的 Create 按钮，在弹出的浮窗里输入项目的名称和爬取的链接，再点击 Create 按钮，这样就成功创建了一个项目

接下来会看到 pyspider 的项目编辑和调试页面

左侧就是代码的调试页面，点击左侧右上角的 run 单步调试爬虫程序，在左侧下半部分可以预览当前的爬取页面。右侧是代码编辑页面，我们可以直接编辑代码和保存代码，不需要借助于 IDE。

注意右侧，pyspider 已经帮我们生成了一段代码，代码如下所示：
```python
from pyspider.libs.base_handler import *
 
class Handler(BaseHandler):
    crawl_config = { }
 
    @every(minutes=24 * 60)
    def on_start(self):
        self.crawl('http://travel.qunar.com/travelbook/list.htm', callback=self.index_page)
 
    @config(age=10 * 24 * 60 * 60)
    def index_page(self, response):
        for each in response.doc('a[href^="http"]').items():
            self.crawl(each.attr.href, callback=self.detail_page)
 
    @config(priority=2)
    def detail_page(self, response):
        return {
            "url": response.url,
            "title": response.doc('title').text(),}
```
这里的 Handler 就是 pyspider 爬虫的主类，我们可以在此处定义爬取、解析、存储的逻辑。整个爬虫的功能只需要一个 Handler 即可完成。

接下来我们可以看到一个 crawl_config 属性。我们可以将本项目的所有爬取配置统一定义到这里，如定义 Headers、设置代理等，配置之后全局生效。

然后，on_start() 方法是爬取入口，初始的爬取请求会在这里产生，该方法通过调用 crawl() 方法即可新建一个爬取请求，第一个参数是爬取的 URL，这里自动替换成我们所定义的 URL。crawl() 方法还有一个参数 callback，它指定了这个页面爬取成功后用哪个方法进行解析，代码中指定为 index_page() 方法，即如果这个 URL 对应的页面爬取成功了，那 Response 将交给 index_page() 方法解析。

index_page() 方法恰好接收这个 Response 参数，Response 对接了 pyquery。我们直接调用 doc() 方法传入相应的 CSS 选择器，就可以像 pyquery 一样解析此页面，代码中默认是 a\[href^="http"]，也就是说该方法解析了页面的所有链接，然后将链接遍历，再次调用了 crawl() 方法生成了新的爬取请求，同时再指定了 callback 为 detail_page，意思是说这些页面爬取成功了就调用 detail_page() 方法解析。这里，index_page() 实现了两个功能，一是将爬取的结果进行解析，二是生成新的爬取请求。

detail_page() 同样接收 Response 作为参数。detail_page() 抓取的就是详情页的信息，就不会生成新的请求，只对 Response 对象做解析，解析之后将结果以字典的形式返回。当然我们也可以进行后续处理，如将结果保存到数据库。
### 爬取首页

点击左栏右上角的 run 按钮，即可看到页面下方 follows 便会出现一个标注，其中包含数字 1，这代表有新的爬取请求产生

左栏左上角会出现当前 run 的配置文件，这里有一个 callback 为 on_start，这说明点击 run 之后实际是执行了 on_start() 方法。在 on_start() 方法中，我们利用 crawl() 方法生成一个爬取请求，那下方 follows 部分的数字 1 就代表了这一个爬取请求。

点击下方的 follows 按钮，即可看到生成的爬取请求的链接。每个链接的右侧还有一个箭头按钮

点击该箭头，我们就可以对此链接进行爬取，也就是爬取攻略的首页内容

上方的 callback 已经变成了 index_page，这就代表当前运行了 index_page() 方法。index_page() 接收到的 response 参数就是刚才生成的第一个爬取请求的 Response 对象。index_page() 方法通过调用 doc() 方法，传入提取所有 a 节点的 CSS 选择器，然后获取 a 节点的属性 href，这样实际上就是获取了第一个爬取页面中的所有链接。然后在 index_page() 方法里遍历了所有链接，同时调用 crawl() 方法，就把这一个个的链接构造成新的爬取请求了。所以最下方 follows 按钮部分有 217 的数字标记，这代表新生成了 217 个爬取请求，同时这些请求的 URL 都呈现在当前页面了。

再点击下方的 web 按钮，即可预览当前爬取结果的页面

当前看到的页面结果和浏览器看到的几乎是完全一致的，在这里我们可以方便地查看页面请求的结果。

点击 html 按钮即可查看当前页面的源代码

如果需要分析代码的结构，我们可以直接参考页面源码。

我们刚才在 index_page() 方法中提取了所有的链接并生成了新的爬取请求。但是很明显要爬取的肯定不是所有链接，只需要攻略详情的页面链接就够了，所以我们要修改一下当前 index_page() 里提取链接时的 CSS 选择器。

接下来需要另外一个工具。首先切换到 Web 页面，找到攻略的标题，点击下方的 enable css selector helper，点击标题。这时候我们看到标题外多了一个红框，上方出现了一个 CSS 选择器，这就是当前标题对应的 CSS 选择器

在右侧代码选中要更改的区域，点击左栏的右箭头，此时在上方出现的标题的 CSS 选择器就会被替换到右侧代码中

这样就成功完成了 CSS 选择器的替换，非常便捷。

重新点击左栏右上角的 run 按钮，即可重新执行 index_page() 方法。此时的 follows 就变成了 10 个，也就是说现在我们提取的只有当前页面的 10 个攻略

我们现在抓取的只是第一页的内容，还需要抓取后续页面，所以还需要一个爬取链接，即爬取下一页的攻略列表页面。我们再利用 crawl() 方法添加下一页的爬取请求，在 index_page() 方法里面添加如下代码，然后点击 save 保存：
```python
next = response.doc('.next').attr.href
self.crawl(next, callback=self.index_page)
```
利用 CSS 选择器选中下一页的链接，获取它的 href 属性，也就获取了页面的 URL。然后将该 URL 传给 crawl() 方法，同时指定回调函数，注意这里回调函数仍然指定为 index_page() 方法，因为下一页的结构与此页相同。

重新点击 run 按钮，这时就可以看到 11 个爬取请求。follows 按钮上会显示 11，这就代表我们成功添加了下一页的爬取请求

现在，索引列表页的解析过程我们就完成了。

### 爬取详情页

任意选取一个详情页进入，点击前 10 个爬取请求中的任意一个的右箭头，执行详情页的爬取

切换到 Web 页面预览效果，页面下拉之后，头图正文中的一些图片一直显示加载中

查看源代码，我们没有看到 img 节点

出现此现象的原因是 pyspider 默认发送 HTTP 请求，请求的 HTML 文档本身就不包含 img 节点。但是在浏览器中我们看到了图片，这是因为这张图片是后期经过 JavaScript 出现的。那么，我们该如何获取呢？

幸运的是，pyspider 内部对接了 PhantomJS，那么我们只需要修改一个参数即可。

我们将 index_page() 中生成抓取详情页的请求方法添加一个参数 fetch_type，改写的 index_page() 变为如下内容：
```python
def index_page(self, response):
    for each in response.doc('li> .tit > a').items():
        self.crawl(each.attr.href, callback=self.detail_page, fetch_type='js')
    next = response.doc('.next').attr.href
    self.crawl(next, callback=self.index_page)
```
接下来，我们来试试它的抓取效果。

点击左栏上方的左箭头返回，重新调用 index_page() 方法生成新的爬取详情页的 Request

再点击新生成的详情页 Request 的爬取按钮，这时我们便可以看到页面变成了这样子

图片被成功渲染出来，这就是启用了 PhantomJS 渲染后的结果。只需要添加一个 fetch_type 参数即可，这非常方便。

最后就是将详情页中需要的信息提取出来，提取过程不再赘述。最终 detail_page() 方法改写如下所示：
```python
def detail_page(self, response):
    return {
        'url': response.url,
        'title': response.doc('#booktitle').text(),
        'date': response.doc('.when .data').text(),
        'day': response.doc('.howlong .data').text(),
        'who': response.doc('.who .data').text(),
        'text': response.doc('#b_panel_schedule').text(),
        'image': response.doc('.cover_img').attr.src
    }
```
我们分别提取了页面的链接、标题、出行日期、出行天数、人物、攻略正文、头图信息，将这些信息构造成一个字典。

重新运行，即可发现输出结果

左栏中输出了最终构造的字典信息，这就是一篇攻略的抓取结果。

### 启动爬虫

返回爬虫的主页面，将爬虫的 status 设置成 DEBUG 或 RUNNING，点击右侧的 Run 按钮即可开始爬取

在最左侧我们可以定义项目的分组，以方便管理。rate/burst 代表当前的爬取速率，rate 代表 1 秒发出多少个请求，burst 相当于流量控制中的令牌桶算法的令牌数，rate 和 burst 设置的越大，爬取速率越快，当然速率需要考虑本机性能和爬取过快被封的问题。process 中的 5m、1h、1d 指的是最近 5 分、1 小时、1 天内的请求情况，all 代表所有的请求情况。请求由不同颜色表示，蓝色的代表等待被执行的请求，绿色的代表成功的请求，黄色的代表请求失败后等待重试的请求，红色的代表失败次数过多而被忽略的请求，这样可以直观知道爬取的进度和请求情况

点击 Active Tasks，即可查看最近请求的详细状况

点击 Results，即可查看所有的爬取结果

点击右上角的按钮，即可获取数据的 JSON、CSV 格式。

## 用法详解
### 命令行

上面的实例通过如下命令启动 pyspider：
```bash
pyspider all
```
命令行还有很多可配制参数，完整的命令行结构如下所示：
```bash
pyspider [OPTIONS] COMMAND [ARGS]
```
其中，OPTIONS 为可选参数，它可以指定如下参数。
```bash
Options:
  -c, --config FILENAME    指定配置文件名称
  --logging-config TEXT    日志配置文件名称，默认: pyspider/pyspider/logging.conf
  --debug                  开启调试模式
  --queue-maxsize INTEGER  队列的最大长度
  --taskdb TEXT            taskdb 的数据库连接字符串，默认: sqlite
  --projectdb TEXT         projectdb 的数据库连接字符串，默认: sqlite
  --resultdb TEXT          resultdb 的数据库连接字符串，默认: sqlite
  --message-queue TEXT     消息队列连接字符串，默认: multiprocessing.Queue
  --phantomjs-proxy TEXT   PhantomJS 使用的代理，ip:port 的形式
  --data-path TEXT         数据库存放的路径
  --version                pyspider 的版本
  --help                   显示帮助信息
```
例如，-c 可以指定配置文件的名称，这是一个常用的配置，配置文件的样例结构如下所示：
```bash
{
  "taskdb": "mysql+taskdb://username:password@host:port/taskdb",
  "projectdb": "mysql+projectdb://username:password@host:port/projectdb",
  "resultdb": "mysql+resultdb://username:password@host:port/resultdb",
  "message_queue": "amqp://username:password@host:port/%2F",
  "webui": {
    "username": "some_name",
    "password": "some_passwd",
    "need-auth": true
  }
}
```
如果要配置 pyspider WebUI 的访问认证，可以新建一个 pyspider.json，内容如下所示：
```bash
{
  "webui": {
    "username": "root",
    "password": "123456",
    "need-auth": true
  }
}
```
这样我们通过在启动时指定配置文件来配置 pyspider WebUI 的访问认证，用户名为 root，密码为 123456，命令如下所示：
	
pyspider -c pyspider.json all

运行之后打开：http://localhost:5000/

也可以单独运行 pyspider 的某一个组件。

运行 Scheduler 的命令如下所示：
```bash
pyspider scheduler [OPTIONS]
```
运行时也可以指定各种配置，参数如下所示：
```bash
Options:
  --xmlrpc /--no-xmlrpc
  --xmlrpc-host TEXT
  --xmlrpc-port INTEGER
  --inqueue-limit INTEGER  任务队列的最大长度，如果满了则新的任务会被忽略
  --delete-time INTEGER    设置为 delete 标记之前的删除时间
  --active-tasks INTEGER   当前活跃任务数量配置
  --loop-limit INTEGER     单轮最多调度的任务数量
  --scheduler-cls TEXT     Scheduler 使用的类
  --help                   显示帮助信息
```
运行 Fetcher 的命令如下所示：
```bash
pyspider fetcher [OPTIONS]
```
参数配置如下所示：
```bash
Options:
  --xmlrpc /--no-xmlrpc
  --xmlrpc-host TEXT
  --xmlrpc-port INTEGER
  --poolsize INTEGER      同时请求的个数
  --proxy TEXT            使用的代理
  --user-agent TEXT       使用的 User-Agent
  --timeout TEXT          超时时间
  --fetcher-cls TEXT      Fetcher 使用的类
  --help                  显示帮助信息
```
运行 Processer 的命令如下所示：
```bash
pyspider processor [OPTIONS]
```
参数配置如下所示：
```bash	
Options:
  --processor-cls TEXT  Processor 使用的类
  --help                显示帮助信息
```
运行 WebUI 的命令如下所示：
```bash	
pyspider webui [OPTIONS]
```
参数配置如下所示：
```bash
Options:
  --host TEXT            运行地址
  --port INTEGER         运行端口
  --cdn TEXT             JS 和 CSS 的 CDN 服务器
  --scheduler-rpc TEXT   Scheduler 的 xmlrpc 路径
  --fetcher-rpc TEXT     Fetcher 的 xmlrpc 路径
  --max-rate FLOAT       每个项目最大的 rate 值
  --max-burst FLOAT      每个项目最大的 burst 值
  --username TEXT        Auth 验证的用户名
  --password TEXT        Auth 验证的密码
  --need-auth            是否需要验证
  --webui-instance TEXT  运行时使用的 Flask 应用
  --help                 显示帮助信息
```
这里的配置和前面提到的配置文件参数是相同的。如果想要改变 WebUI 的端口为 5001，单独运行如下命令：
```bash
pyspider webui --port 5001
```
或者可以将端口配置到 JSON 文件中，配置如下所示：
```bash
{
  "webui": {"port": 5001}
}
```
使用如下命令启动同样可以达到相同的效果：
```bash
pyspider -c pyspider.json webui
```
这样就可以在 5001 端口上运行 WebUI 了。

### crawl() 方法

在前面的例子中，我们使用 crawl() 方法实现了新请求的生成，但是只指定了 URL 和 Callback。这里将详细介绍一下 crawl() 方法的参数配置。
- url

  url 是爬取时的 URL，可以定义为单个 URL 字符串，也可以定义成 URL 列表。

- callback

  callback 是回调函数，指定了该 URL 对应的响应内容用哪个方法来解析，如下所示：
  ```python 
  def on_start(self):
      self.crawl('http://scrapy.org/', callback=self.index_page)
  ```
  这里指定了 callback 为 index_page，就代表爬取 http://scrapy.org/ 链接得到的响应会用 index_page() 方法来解析。

  index_page() 方法的第一个参数是响应对象，如下所示：
  ```python
  def index_page(self, response):
      pass
  ```
  方法中的 response 参数就是请求上述 URL 得到的响应对象，我们可以直接在 index_page() 方法中实现页面的解析。

- age

  age 是任务的有效时间。如果某个任务在有效时间内且已经被执行，则它不会重复执行，如下所示：
  ```python
  def on_start(self):
      self.crawl('http://www.example.org/', callback=self.callback,
                 age=10*24*60*60)
  ```
  或者可以这样设置：
  ```python
  @config(age=10 * 24 * 60 * 60)
  def callback(self):
      pass
  ```
  默认的有效时间为 10 天。

- priority

  priority 是爬取任务的优先级，其值默认是 0，priority 的数值越大，对应的请求会越优先被调度，如下所示：
  ```python
  def index_page(self):
      self.crawl('http://www.example.org/page.html', callback=self.index_page)
      self.crawl('http://www.example.org/233.html', callback=self.detail_page,
                 priority=1)
  ```
  第二个任务会优先调用，233.html 这个链接优先爬取。

- exetime

  exetime 参数可以设置定时任务，其值是时间戳，默认是 0，即代表立即执行，如下所示：
  ```python
  import time
  def on_start(self):
      self.crawl('http://www.example.org/', callback=self.callback,
                 exetime=time.time()+30*60)
  ```
  这样该任务会在 30 分钟之后执行。

- retries

  retries 可以定义重试次数，其值默认是 3。

- itag

  itag 参数设置判定网页是否发生变化的节点值，在爬取时会判定次当前节点是否和上次爬取到的节点相同。如果节点相同，则证明页面没有更新，就不会重复爬取，如下所示：
  ```python	
  def index_page(self, response):
      for item in response.doc('.item').items():
          self.crawl(item.find('a').attr.url, callback=self.detail_page,
                     itag=item.find('.update-time').text())
  ```
  在这里设置了更新时间这个节点的值为 itag，在下次爬取时就会首先检测这个值有没有发生变化，如果没有变化，则不再重复爬取，否则执行爬取。

- auto_recrawl

  当开启时，爬取任务在过期后会重新执行，循环时间即定义的 age 时间长度，如下所示：
  ```python
  def on_start(self):
      self.crawl('http://www.example.org/', callback=self.callback,
                 age=5*60*60, auto_recrawl=True)
  ```
  这里定义了 age 有效期为 5 小时，设置了 auto_recrawl 为 True，这样任务就会每 5 小时执行一次。

- method

  method 是 HTTP 请求方式，它默认是 GET。如果想发起 POST 请求，可以将 method 设置为 POST。

- params

  我们可以方便地使用 params 来定义 GET 请求参数，如下所示：
  ```python
  def on_start(self):
      self.crawl('http://httpbin.org/get', callback=self.callback,
                 params={'a': 123, 'b': 'c'})
      self.crawl('http://httpbin.org/get?a=123&b=c', callback=self.callback)
  ```
  这里两个爬取任务是等价的。

- data

  data 是 POST 表单数据。当请求方式为 POST 时，我们可以通过此参数传递表单数据，如下所示：
  ```python
  def on_start(self):
      self.crawl('http://httpbin.org/post', callback=self.callback,
                 method='POST', data={'a': 123, 'b': 'c'})
  ```

- files

  files 是上传的文件，需要指定文件名，如下所示：
  ```python
  def on_start(self):
      self.crawl('http://httpbin.org/post', callback=self.callback,
                 method='POST', files={field: {filename: 'content'}})
  ```

- user_agent

  user_agent 是爬取使用的 User-Agent。

- headers

  headers 是爬取时使用的 Headers，即 Request Headers。

- cookies

  cookies 是爬取时使用的 Cookies，为字典格式。

- connect_timeout

  connect_timeout 是在初始化连接时的最长等待时间，它默认是 20 秒。

- timeout

  timeout 是抓取网页时的最长等待时间，它默认是 120 秒。

- allow_redirects

  allow_redirects 确定是否自动处理重定向，它默认是 True。

- validate_cert

  validate_cert 确定是否验证证书，此选项对 HTTPS 请求有效，默认是 True。

- proxy

  proxy 是爬取时使用的代理，它支持用户名密码的配置，格式为 username:password@hostname:port，如下所示：
  ```python
  def on_start(self):
      self.crawl('http://httpbin.org/get', callback=self.callback, proxy='127.0.0.1:9743')
  ```
  也可以设置 craw_config 来实现全局配置，如下所示：
  ```python
  class Handler(BaseHandler):
      crawl_config = {'proxy': '127.0.0.1:9743'}
  ```

- fetch_type

  fetch_type 开启 PhantomJS 渲染。如果遇到 JavaScript 渲染的页面，指定此字段即可实现 PhantomJS 的对接，pyspider 将会使用 PhantomJS 进行网页的抓取，如下所示：
  ```python
  def on_start(self):
      self.crawl('https://www.taobao.com', callback=self.index_page, fetch_type='js')
  ```
  这样我们就可以实现淘宝页面的抓取了，得到的结果就是浏览器中看到的效果。

- js_script

  js_script 是页面加载完毕后执行的 JavaScript 脚本，如下所示：
  ```python
  def on_start(self):
      self.crawl('http://www.example.org/', callback=self.callback,
                 fetch_type='js', js_script='''
                 function() {window.scrollTo(0,document.body.scrollHeight);
                     return 123;
                 }
                 ''')
  ```
  页面加载成功后将执行页面混动的 JavaScript 代码，页面会下拉到最底部。

- js_run_at

  js_run_at 代表 JavaScript 脚本运行的位置，是在页面节点开头还是结尾，默认是结尾，即 document-end。

- js_viewport_width/js_viewport_height

  js_viewport_width/js_viewport_height 是 JavaScript 渲染页面时的窗口大小。

- load_images

  load_images 在加载 JavaScript 页面时确定是否加载图片，它默认是否。

- save

  save 参数非常有用，可以在不同的方法之间传递参数，如下所示：
  ```python
  def on_start(self):
      self.crawl('http://www.example.org/', callback=self.callback,
                 save={'page': 1})
   
  def callback(self, response):
      return response.save['page']
  ```
  这样，在 on_start() 方法中生成 Request 并传递额外的参数 page，在回调函数里可以通过 response 变量的 save 字段接收到这些参数值。

- cancel

  cancel 是取消任务，如果一个任务是 ACTIVE 状态的，则需要将 force_update 设置为 True。
  force_update

  即使任务处于 ACTIVE 状态，那也会强制更新状态。

  以上便是 crawl() 方法的参数介绍，更加详细的描述可以参考： http://docs.pyspider.org/en/latest/apis/self.crawl/。

### 任务区分

在 pyspider 判断两个任务是否是重复的是使用的是该任务对应的 URL 的 MD5 值作为任务的唯一 ID，如果 ID 相同，那么两个任务就会判定为相同，其中一个就不会爬取了。很多情况下请求的链接可能是同一个，但是 POST 的参数不同。这时可以重写 task_id() 方法，改变这个 ID 的计算方式来实现不同任务的区分，如下所示：
```python
import json
from pyspider.libs.utils import md5string
def get_taskid(self, task):
    return md5string(task['url']+json.dumps(task['fetch'].get('data', '')))
```
这里重写了 get_taskid() 方法，利用 URL 和 POST 的参数来生成 ID。这样一来，即使 URL 相同，但是 POST 的参数不同，两个任务的 ID 就不同，它们就不会被识别成重复任务。

### 全局配置

pyspider 可以使用 crawl_config 来指定全局的配置，配置中的参数会和 crawl() 方法创建任务时的参数合并。如要全局配置一个 Headers，可以定义如下代码：
```python
class Handler(BaseHandler):
    crawl_config = {
        'headers': {'User-Agent': 'GoogleBot',}
    }
```
### 定时爬取

我们可以通过 every 属性来设置爬取的时间间隔，如下所示：
```python
@every(minutes=24 * 60)
def on_start(self):
    for url in urllist:
        self.crawl(url, callback=self.index_page)
```
这里设置了每天执行一次爬取。

在上文中我们提到了任务的有效时间，在有效时间内爬取不会重复。所以要把有效时间设置得比重复时间更短，这样才可以实现定时爬取。

例如，下面的代码就无法做到每天爬取：
```python
@every(minutes=24 * 60)
def on_start(self):
    self.crawl('http://www.example.org/', callback=self.index_page)
 
@config(age=10 * 24 * 60 * 60)
def index_page(self):
    pass
```
这里任务的过期时间为 10 天，而自动爬取的时间间隔为 1 天。当第二次尝试重新爬取的时候，pyspider 会监测到此任务尚未过期，便不会执行爬取，所以我们需要将 age 设置得小于定时时间。

### 项目状态

每个项目都有 6 个状态，分别是 TODO、STOP、CHECKING、DEBUG、RUNNING、PAUSE。

- TODO：它是项目刚刚被创建还未实现时的状态。
- STOP：如果想停止某项目的抓取，可以将项目的状态设置为 STOP。
- CHECKING：正在运行的项目被修改后就会变成 CHECKING 状态，项目在中途出错需要调整的时候会遇到这种情况。
- DEBUG/RUNNING：这两个状态对项目的运行没有影响，状态设置为任意一个，项目都可以运行，但是可以用二者来区分项目是否已经测试通过。
- PAUSE：当爬取过程中出现连续多次错误时，项目会自动设置为 PAUSE 状态，并等待一定时间后继续爬取。

### 爬取进度

在抓取时，可以看到抓取的进度，progress 部分会显示 4 个进度条

progress 中的 5m、1h、1d 指的是最近 5 分、1 小时、1 天内的请求情况，all 代表所有的请求情况。

蓝色的请求代表等待被执行的任务，绿色的代表成功的任务，黄色的代表请求失败后等待重试的任务，红色的代表失败次数过多而被忽略的任务，从这里我们可以直观看到爬取的进度和请求情况。

### 删除项目

pyspider 中没有直接删除项目的选项。如要删除任务，那么将项目的状态设置为 STOP，将分组的名称设置为 delete，等待 24 小时，则项目会自动删除。
