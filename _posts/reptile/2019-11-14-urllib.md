---
layout:     post
title:      "urllib"
subtitle:   "urllib的详解使用"
date:       2019-11-14 14:36:25
author:     "Balbo"
header-img: "img/post-bg-2019.jpg"
tags:
    - reptile
    - urllib
---
我们首先了解一下 Urllib 库，它是 Python 内置的 HTTP 请求库，也就是说我们不需要额外安装即可使用，它包含四个模块：

- 第一个模块 request，它是最基本的 HTTP 请求模块，我们可以用它来模拟发送一请求，就像在浏览器里输入网址然后敲击回车一样，只需要给库方法传入 URL 还有额外的参数，就可以模拟实现这个过程了。
- 第二个 error 模块即异常处理模块，如果出现请求错误，我们可以捕获这些异常，然后进行重试或其他操作保证程序不会意外终止。
- 第三个 parse 模块是一个工具模块，提供了许多 URL 处理方法，比如拆分、解析、合并等等的方法。
- 第四个模块 robotparser，主要是用来识别网站的 robots.txt 文件，然后判断哪些网站可以爬，哪些网站不可以爬的，其实用的比较少。

    在这里重点对前三个模块进行下讲解。

## 发送请求

使用 Urllib 的 request 模块我们可以方便地实现 Request 的发送并得到 Response

### urlopen()

urllib.request 模块提供了最基本的构造 HTTP 请求的方法，利用它可以模拟浏览器的一个请求发起过程，同时它还带有处理  authenticaton（授权验证），redirections（重定向)，cookies（浏览器Cookies）以及其它内容。
我们来感受一下它的强大之处，以 Python 官网为例，我们来把这个网页抓下来：
```
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(response.read().decode('utf-8'))
```
运行结果如下：
```html
<!doctype html>
<!--[if lt IE 7]>   <html class="no-js ie6 lt-ie7 lt-ie8 lt-ie9">   <![endif]-->
<!--[if IE 7]>      <html class="no-js ie7 lt-ie8 lt-ie9">          <![endif]-->
<!--[if IE 8]>      <html class="no-js ie8 lt-ie9">                 <![endif]-->
<!--[if gt IE 8]><!--><html class="no-js" lang="en" dir="ltr">  <!--<![endif]-->
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link rel="prefetch" href="//ajax.googleapis.com/ajax/libs/jquery/1.8.2/jquery.min.js">
    <meta name="application-name" content="Python.org">
    <meta name="msapplication-tooltip" content="The official home of the Python Programming Language">
    <meta name="apple-mobile-web-app-title" content="Python.org">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="HandheldFriendly" content="True">
    <meta name="format-detection" content="telephone=no">
    <meta http-equiv="cleartype" content="on">
    <meta http-equiv="imagetoolbar" content="false">
    <script src="/static/js/libs/modernizr.js"></script>
    <link href="/static/stylesheets/style.67f4b30f7483.css" rel="stylesheet" type="text/css" title="default" />
    <link href="/static/stylesheets/mq.3ae8e02ece5b.css" rel="stylesheet" type="text/css" media="not print, braille, embossed, speech, tty" />

    ...

</html>
```

接下来我们看下它返回的到底是什么，利用 type() 方法输出 Response 的类型。
```python
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(type(response))
```
输出结果如下：
```html
<class 'http.client.HTTPResponse'>
```
通过输出结果可以发现它是一个 HTTPResposne 类型的对象，它主要包含的方法有 read()、readinto()、getheader(name)、getheaders()、fileno() 等方法和 msg、version、status、reason、debuglevel、closed 等属性。

得到这个对象之后，我们把它赋值为 response 变量，然后就可以调用这些方法和属性，得到返回结果的一系列信息了。+

例如调用 read() 方法可以得到返回的网页内容，调用 status 属性就可以得到返回结果的状态码，如 200 代表请求成功，404 代表网页未找到等。

下面再来一个实例感受一下：
```python
import urllib.request

response = urllib.request.urlopen('https://www.python.org')
print(response.status)
print(response.getheaders())
print(response.getheader('Server'))
```
运行结果如下：
```
200
[('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8'), ('X-Frame-Options', 'SAMEORIGIN'), ('X-Clacks-Overhead', 'GNU Terry Pratchett'), ('Content-Length', '47397'), ('Accept-Ranges', 'bytes'), ('Date', 'Mon, 01 Aug 2016 09:57:31 GMT'), ('Via', '1.1 varnish'), ('Age', '2473'), ('Connection', 'close'), ('X-Served-By', 'cache-lcy1125-LCY'), ('X-Cache', 'HIT'), ('X-Cache-Hits', '23'), ('Vary', 'Cookie'), ('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]
nginx
```
可见，三个输出分别输出了响应的状态码，响应的头信息，以及通过调用 getheader() 方法并传递一个参数 Server 获取了 headers 中的 Server 值，结果是 nginx，意思就是服务器是 nginx 搭建的。

利用以上最基本的 urlopen() 方法，我们可以完成最基本的简单网页的 GET 请求抓取。
如果我们想给链接传递一些参数该怎么实现呢？我们首先看一下 urlopen() 函数的API：
```
urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)
```
可以发现除了第一个参数可以传递 URL 之外，我们还可以传递其它的内容，比如 data（附加数据）、timeout（超时时间）等等。

下面我们详细说明下这几个参数的用法:

- data参数

    data 参数是可选的，如果要添加 data，它要是字节流编码格式的内容，即 bytes 类型，通过 bytes() 方法可以进行转化，另外如果传递了这个 data 参数，它的请求方式就不再是 GET 方式请求，而是 POST。

    下面用一个实例来感受一下：
    ```python
    import urllib.parse
    import urllib.request

    data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')
    response = urllib.request.urlopen('http://httpbin.org/post', data=data)
    print(response.read())
    ```
    在这里我们传递了一个参数 word，值是 hello。它需要被转码成bytes（字节流）类型。其中转字节流采用了 bytes() 方法，第一个参数需要是 str（字符串）类型，需要用 urllib.parse 模块里的 urlencode() 方法来将参数字典转化为字符串。第二个参数指定编码格式，在这里指定为 utf8。

- timeout参数

    timeout 参数可以设置超时时间，单位为秒，意思就是如果请求超出了设置的这个时间还没有得到响应，就会抛出异常，如果不指定，就会使用全局默认时间。它支持 HTTP、HTTPS、FTP 请求。

    因此我们可以通过设置这个超时时间来控制一个网页如果长时间未响应就跳过它的抓取，利用 try except 语句就可以实现这样的操作，代码如下：
    ```python
    import socket
    import urllib.request
    import urllib.error

    try:
        response = urllib.request.urlopen('http://httpbin.org/get', timeout=0.1)
    except urllib.error.URLError as e:
        if isinstance(e.reason, socket.timeout):
            print('TIME OUT')
    ```

- 其他参数

    还有 context 参数，它必须是 ssl.SSLContext 类型，用来指定 SSL 设置。

    cafile 和 capath 两个参数是指定 CA 证书和它的路径，这个在请求 HTTPS 链接时会有用。
    cadefault 参数现在已经弃用了，默认为 False。

    以上讲解了 urlopen() 方法的用法，通过这个最基本的函数可以完成简单的请求和网页抓取，如需更加详细了解，可以参见官方文档：https://docs.python.org/3/library/urllib.request.html。

### Request

由上我们知道利用 urlopen() 方法可以实现最基本请求的发起，但这几个简单的参数并不足以构建一个完整的请求，如果请求中需要加入 Headers 等信息，我们就可以利用更强大的 Request 类来构建一个请求。
首先我们用一个实例来感受一下 Request 的用法：
```python
import urllib.request

request = urllib.request.Request('https://python.org')
response = urllib.request.urlopen(request)
print(response.read().decode('utf-8'))
```
可以发现，我们依然是用 urlopen() 方法来发送这个请求，只不过这次 urlopen() 方法的参数不再是一个 URL，而是一个 Request 类型的对象，通过构造这个这个数据结构，一方面我们可以将请求独立成一个对象，另一方面可配置参数更加丰富和灵活。
下面我们看一下 Request 都可以通过怎样的参数来构造，它的构造方法如下：
```
class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
```
1. 第一个 url 参数是请求 URL，这个是必传参数，其他的都是可选参数。
2. 第二个 data 参数如果要传必须传 bytes（字节流）类型的，如果是一个字典，可以先用 urllib.parse 模块里的 urlencode() 编码。
3. 第三个 headers 参数是一个字典，这个就是 Request Headers 了，你可以在构造 Request 时通过 headers 参数直接构造，也可以通过调用 Request 实例的 add_header() 方法来添加, Request Headers 最常用的用法就是通过修改 User-Agent 来伪装浏览器，默认的 User-Agent 是 Python-urllib，我们可以通过修改它来伪装浏览器。
4. 第四个 origin_req_host 参数指的是请求方的 host 名称或者 IP 地址。
5. 第五个 unverifiable 参数指的是这个请求是否是无法验证的，默认是False。意思就是说用户没有足够权限来选择接收这个请求的结果。例如我们请求一个 HTML 文档中的图片，但是我们没有自动抓取图像的权限，这时 unverifiable 的值就是 True。
6. 第六个 method 参数是一个字符串，它用来指示请求使用的方法，比如GET，POST，PUT等等。

写个例子：
```python
from urllib import request, parse

url = 'http://httpbin.org/post'
headers = {
    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',
    'Host': 'httpbin.org'
}
dict = {
    'name': 'Germey'
}
data = bytes(parse.urlencode(dict), encoding='utf8')
req = request.Request(url=url, data=data, headers=headers, method='POST')
response = request.urlopen(req)
print(response.read().decode('utf-8'))
```
在这里我们通过四个参数构造了一个 Request，url 即请求 URL，在headers 中指定了 User-Agent 和 Host，传递的参数 data 用了 urlencode() 和 bytes() 方法来转成字节流，另外指定了请求方式为 POST。

通过观察结果可以发现，我们成功设置了 data，headers 以及 method。
另外 headers 也可以用 add_header() 方法来添加。
```python
req = request.Request(url=url, data=data, method='POST')
req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')
```
如此一来，我们就可以更加方便地构造一个 Request，实现请求的发送啦。
## 处理异常

我们了解了 Request 的发送过程，但是在网络情况不好的情况下，出现了异常怎么办呢？这时如果我们不处理这些异常，程序很可能报错而终止运行，所以异常处理还是十分有必要的。

Urllib 的 error 模块定义了由 request 模块产生的异常。如果出现了问题，request 模块便会抛出 error 模块中定义的异常。

主要有这两个处理异常类，URLError,HTTPError
下面写个例子：
```python
from urllib import request, error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.URLError as e:
    print(e.reason)
```
我们打开一个不存在的页面，照理来说应该会报错，但是这时我们捕获了 URLError 这个异常，运行结果如下：
```
Not Found
```
```python
from urllib import request,error
try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, seq='\n')
```
运行结果：
```
Not Found
404
Server: nginx/1.4.6 (Ubuntu)
Date: Wed, 03 Aug 2016 08:54:22 GMT
Content-Type: text/html; charset=UTF-8
Transfer-Encoding: chunked
Connection: close
X-Powered-By: PHP/5.5.9-1ubuntu4.14
Vary: Cookie
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Cache-Control: no-cache, must-revalidate, max-age=0
Pragma: no-cache
Link: <http://cuiqingcai.com/wp-json/>; rel="https://api.w.org/"
```
HTTPError,它有三个属性。

- code，返回 HTTP Status Code，即状态码，比如 404 网页不存在，500 服务器内部错误等等。
- reason，同父类一样，返回错误的原因。
- headers，返回 Request Headers。
- 因为 URLError 是 HTTPError 的父类，所以我们可以先选择捕获子类的错误，再去捕获父类的错误，所以上述代码更好的写法如下：
```python
from urllib import request, error

try:
    response = request.urlopen('http://cuiqingcai.com/index.htm')
except error.HTTPError as e:
    print(e.reason, e.code, e.headers, sep='\n')
except error.URLError as e:
    print(e.reason)
else:
    print('Request Successfully')
```
这样我们就可以做到先捕获 HTTPError，获取它的错误状态码、原因、Headers 等详细信息。如果非 HTTPError，再捕获 URLError 异常，输出错误原因。最后用 else 来处理正常的逻辑，这是一个较好的异常处理写法。

有时，reason 属性返回的不一定是字符串，也可能是一个对象。
```python
import socket
from urllib import request, error

try:
    response = request.urlopen('https://www.baidu.com', timeout=0.01)
except error.URLError as e:
    print(type(e.reason))
    if isinstance(e.reason, socket.timeout):
        print('TIME OUT')
```

## 解析链接
前面说过，urllib 库里还提供了 parse 模块，他定义了处理 URL 的标准接口，例如实现 URL 各部分的抽取、合并以及链接转换。他支持如下协议的 URL 处理：file、gopher、hdl、http、https、imap、mailto、mms、news、nntp、prospero、rsync、rtsp、rtspu、sftp、sip、sips、snews、svn、svn+ssh、telnet 和 wais。我们介绍一下改模块中常用的方法来看一下他的便捷之处：

### urlparse()

该方法可以实现 URL 的识别和分段
```python
from urllib.parse import urlparse

result = urlparse('https://www.baidu.com/index.html;user?id=5%comment')
print(result)
```
运行结果：
```
ParseResult(scheme='https', netloc='www.baidu.com', path='/index.html', params='user', query='id=5%comment', fragment='')
```
由上诉例子，可以得出一个标准的链接格式：
```
scheme://netloc/path;params?query#fragment
```

### urlsplit()

```python
from urllib.parse import urlsplit

result = urlsplit('https://www.baidu.com/index.html;user?id=5%comment')
print(result)
```
运行结果：
```
SplitResult(scheme='https', netloc='www.baidu.com', path='/index.html;user', query='id=5%comment', fragment='')
```
SplitResult 其实是一个元组类型，既可以用属性获取值，也可以用索引来获取
```python
from urllib.parse import urlsplit

result = urlsplit('https://www.baidu.com/index.html;user?id=5%comment')
print(result.scheme, result[0])
```
运行结果：
```
https https
```

### urlunparse()、urlunsplit()

有了 urlparse()，相应的就有了呀的对立方法 urlunparse()。
```python
from urllib.parse import urlunparse

data = ['https', 'www.baidu.com', 'index.html', 'user', 'a=6', 'comment']
print(urlunparse(data))
```
运行结果：
```
https://www.baidu.com/index.html;user?a=6#comment
```
urlunsplit() 和 urlunparse()用法相同，只不过 urlunsplit() 中 params 合并到 path 中。

### urljoin()

这个的功能其实是做拼接的，例子如下：
```python
from urllib.parse import urljoin

print(urljoin('http://www.baidu.com', 'FAQ.html'))
print(urljoin('http://www.baidu.com', 'https://pythonsite.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://pythonsite.com/FAQ.html'))
print(urljoin('http://www.baidu.com/about.html', 'https://pythonsite.com/FAQ.html?question=2'))
print(urljoin('http://www.baidu.com?wd=abc', 'https://pythonsite.com/index.php'))
print(urljoin('http://www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com', '?category=2#comment'))
print(urljoin('www.baidu.com#comment', '?category=2'))
```
运行结果：
```
http://www.baidu.com/FAQ.html
https://pythonsite.com/FAQ.html
https://pythonsite.com/FAQ.html
https://pythonsite.com/FAQ.html?question=2
https://pythonsite.com/index.php
http://www.baidu.com?category=2#comment
www.baidu.com?category=2#comment
www.baidu.com?category=2
```
从拼接的结果我们可以看出，拼接的时候后面的优先级高于前面的url

### urllencode()

这个方法可以将字典转换为url参数，例子如下
```python
from urllib.parse import urlencode

params = {
    "name": "zhaofan",
    "age": 23,
}
base_url = "http://www.baidu.com?"

url = base_url+urlencode(params)
print(url)
```
运行结果：
```
http://www.baidu.com?name=zhaofan&age=23
```

### parse_qs()

有了序列化，就有反序列化。如果我们又一串 GET 请求参数，利用 parse_qs() 方法，就可以将他转回字典
```python
from urllib.parse import parse_qs

query = 'name=germey&age=22'
print(parse_qs(query))
```
运行结果：
```
{'name': ['germey'], 'age': ['22']}
```

### parse_qsl()

```python
from urllib.parse import parse_qsl

query = 'name=germey&age=22'
print(parse_qsl(query))
```
运行结果：
```
[('name', 'germey'), ('age', '22')]
```

### quote()

这个方法用于将内容转化成 URL 编码格式，URL 中带有中文参数时，有时会乱码，此时用这个方法可以将中文转化为 URL 编码
```python
from urllib.parse import quote

keyword = '壁纸'
url = 'http://www.baidu.com/s?wd=' + quote(keyword)
print(url)
```
运行结果：
```
http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8
```

### unquote()

```python
from urllib.parse import unquote

url = 'http://www.baidu.com/s?wd=%E5%A3%81%E7%BA%B8'
print(unquote(url))
```
运行结果：
```
http://www.baidu.com/s?wd=壁纸
```
