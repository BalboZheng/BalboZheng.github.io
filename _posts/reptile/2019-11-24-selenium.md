<<<<<<< HEAD
---
layout:     post
title:      "Selenium"
subtitle:   ""
date:       2019-11-24 21:12:21
author:     "Balbo"
header-img: "img/post-bg-2019.jpg"
tags:
    - reptile
---
## Selenium 的使用
### 基本使用

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait

browser = webdriver.Chrome()
try:
    browser.get('http://www.baidu.com')
    input = browser.find_element_by_id('kw')
    input.send_keys('Python')
    input.send_keys(Keys.ENTER)
    wait = WebDriverWait(browser, 10)
    wait.until(EC.presence_of_element_located((By.ID, 'content_left')))
    print(browser.current_url)
    print(browser.get_cookies())
    print(browser.page_source)
finally:
    browser.close()
```
如果用 Selenium 来驱动浏览器加载网页的话，就可以直接拿到 JavaScript 渲染的结果

### 声明浏览器对象

Selenium 支持非常多的浏览器，如 Chrom、Firefox、Edge等，还有 Android、BalckBerry等手机端的浏览器，还有无界面浏览器 PhantomJS

此外，我们可以用如下方式初始化
```python
from selenium import webdriver

browser = webdriver.Chrome()
browser = webdriver.Firefox()
browser = webdriver.Edge()
browser = webdriver.PhantomJS()
browser = webdriver.Safari()
```

### 访问页面

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://taobao.com/')
print(browser.page_source)
browser.close()
```

### 查找结点

- 单个节点

    想要从淘宝首页中提取搜索框这个节点，首先观察他的源代码

    代码实现
```python
    from selenium import webdriver
    
    browser = webdriver.Chrome()
browser.get('http://taobao.com/')
    input_first = browser.find_element_by_id('q')
    input_second = browser.find_element_by_css_selector('#q')
    input_third = browser.find_element_by_xpath('//*[@id="q"]')
    print(input_first, input_second, input_third)
    browser.close()
```
    这里使用3种方式获取输入框，他们返回的结果都一样都是：
    ```
    <selenium.webdriver.remote.webelement.WebElement (session="dd3692957c85930ca38ce1665fa2787f", element="262fbd50-eadb-4b7d-9a25-776e2e54020b")>
    ```
    另外，获取单个节点的方法还可以用
    ```python
    find_element_by_id()
    find_element_by_name()
    find_element_by_xpath()
    find_element_by_link_text()
    find_element_by_partial_link_text()
    find_element_by_tag_name()
    find_element_by_class_name()
    find_element_by_css_selector()
    
    # 通用方法
# 传入两个参数：查找方式和值
    find_element(By.xx, value)
    ```

- 多个节点

    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    browser.get('http://taobao.com/')
    lis = browser.find_elements_by_css_selector('.service-bd li')
    print(lis)
    browser.close()
    ```
    运行结果：
    ```python
    [<selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="5f8d80a7-847a-4213-a46b-1ece5e469075")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="d2bd82d2-ce90-489d-a827-eb9f13c81a05")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="3a27519d-ea9a-4d00-981d-a02bed240586")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="2ad04b30-84a0-43a9-bd48-455194a9ab6c")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="9ab80873-f1e9-4095-9b7b-5cfd63a4bc1c")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="4e8135ce-d1f7-4f16-a211-58307a96703b")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="4c7f9990-88a2-4731-b1cb-2b0e3b423447")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="c6bd9291-e744-4b95-85c0-ff2e9e37d026")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="0651f495-b40c-4236-9553-3ec9fc3cb2ff")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="87e5aaf1-7ace-45aa-ab33-fa283dde15f9")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="4aa4088e-8afc-43c9-8d96-0c367b0f4e32")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="7eb9b09d-cf45-4554-8c86-4c44fff8b089")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="10634f40-9a59-419c-b09f-6eb0817dd213")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="0efb2ecd-f2b1-478c-93da-c22f8cebef9a")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="525855b9-786b-4dd8-86c0-1dc8811d89f0")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="b6dfe37c-730a-4ee1-a436-04b51d26cb1b")>]
    ```

### 节点交互

```python
import time

from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://taobao.com/')
input = browser.find_element_by_id('q')
input.send_keys('iPhone')
time.sleep(1)
input.clear()
input.send_keys('iPad')
button = browser.find_element_by_class_name('btn-search')
button.click()
```
通过上面的方法，我们完成了一些常见的动作操作，更多的操作可以参见官方文档的交互动作介绍： http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement

### 动作链

```python
from selenium import webdriver
from selenium.webdriver import ActionChains

browser = webdriver.Chrome()
url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'
browser.get(url)
browser.switch_to.frame('iframeResult')
source = browser.find_element_by_css_selector('#draggable')
target = browser.find_element_by_css_selector('#droppable')
actions = ActionChains(browser)
actions.drag_and_drop(source, target)
actions.perform()
```
更多的动作链操作可以参见官方文档： http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains

### 执行 JavaScript

对于默写操作，Selenium API 没有提供，比如，下拉进度条
```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.zhihu.com/explore')
browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')
browser.execute_script('alert("To Bottom")')
```

### 获取节点信息

- 获取属性

    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    url = 'http://www.zhihu.com/explore'
    browser.get(url)
    logo = browser.find_element_by_id('zh-top-link-logo')
    print(logo)
    print(logo.get_attribute('class'))
    ```

    运行之后，程序便会驱动浏览器打开知乎页面，然后获取知乎的logo节点，最后打印出它的class
    ```
    <selenium.webdriver.remote.webelement.WebElement (session="e08c0f28d7f44d75ccd50df6bb676104", element="0.7236390660048155-1")>
    zu-top-link-logo
    ```

- 获取文本值

    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    url = 'http://www.zhihu.com/explore'
    browser.get(url)
    input = browser.find_element_by_class_name('zu-top-add-question')
    print(input.text)
    ```
    运行结果：
    ```
    提问
    ```


- 获取id、位置、标签名和大小

    WebElement节点还有一些其他属性，比如id属性可以获取节点id，location属性可以获取该节点在页面中的相对位置，tag_name属性可以获取标签名称，size属性可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的
    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    url = 'http://www.zhihu.com/explore'
    browser.get(url)
    input = browser.find_element_by_class_name('zu-top-add-question')
    print(input.id)
    print(input.location)
    print(input.tag_name)
    print(input.size)
    ```

### 切换 Frame

网页中有一种节点叫作iframe，也就是子Frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。elenium打开页面后，它默认是在父级Frame里面操作，而此时如果页面中还有子Frame，它是不能获取到子Frame里面的节点的。这时就需要使用switch_to.frame()方法来切换Frame。
```python
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException

browser = webdriver.Chrome()
url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'
browser.get(url)
browser.switch_to.frame('iframeResult')
try:
    logo = browser.find_element_by_class_name('logo')
except NoSuchElementException:
    print('No LOGO!')
browser.switch_to.parent_frame()
logo = browser.find_element_by_class_name('logo')
print(logo)
print(logo.text)
```
运行结果：
```
No LOGO!
<selenium.webdriver.remote.webelement.WebElement (session="a79db882fea6cf339ffff2980d14f4ed", element="8cf2debf-78f1-49d5-a689-4a31ae7306de")>
RUNOOB.COM
```
这里首先通过switch_to.frame()方法切换到子Frame里面，然后尝试获取父级Frame里的logo节点（这是不能找到的），如果找不到的话，就会抛出NoSuchElementException异常，异常被捕捉之后，就会输出NO LOGO。接下来，重新切换回父级Frame，然后再次重新获取节点，发现此时可以成功获取了。

所以，当页面中包含子Frame时，如果想获取子Frame中的节点，需要先调用switch_to.frame()方法切换到对应的Frame，然后再进行操作。

### 延时等待

在Selenium中，get()方法会在网页框架加载结束后结束执行，此时如果获取page_source，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。所以，这里需要延时等待一定时间，确保节点已经加载出来。

- 隐式等待

    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    browser.implicitly_wait(10)
    browser.get('http://www.zhihu.com/explore')
    input = browser.find_element_by_class_name('zu-top-ass-question')
    print(input)
    ```

- 显示等待

    ```python
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    browser = webdriver.Chrome()
    browser.get('http://www.taobao.com/')
    wait = WebDriverWait(browser, 10)
    input = wait.until(EC.presence_of_element_located((By.ID, 'q')))
    button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search')))
    print(input, button)
    ```

| 等待条件 | 含义 |
| ---- | ---- |
| title_is | 标题是某内容 |
| title_contains | 标题包含某内容 |
| presence_of_element_located | 节点加载出来，传入定位元组 |
| visibilty_of_element_located | 节点可见，传入定位元组 |
| visibilty_of | 可见，传入节点对象 |
| presence_of_all_elements_loacted | 所有节点加载出来 |
| text_to_be_present_in_element | 某个节点文本包含某文字 |
| text_to_be_present_in_element_value | 某个节点值包含某文字 |
| frame_to_be_available_and_switch_to_it | 加载并切换 |
| invisibility_of_element_located | 节点不可见 |
| element_to_be_clickable | 节点可点击 |
| staleness_of | 判断一个节点是否仍在 DOM，可判断页面是否已经刷新 |
| element_to_be_selected | 节点可选择，传节点对象 |
| element_located_to_be_selected | 节点可选择，传入定位元组 |
| element_selection_state_to_be | 传入节点对象以及状态，相等返回 True，否则返回 False |
| element_located_selection_state_to_be | 传入定位元组以及状态，相等返回 True，否则返回 False |
| alert_is_present | 是否出现警告 |

### 前进和后退

```python
import time

from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.baidu.com/')
browser.get('http://www.taobao.com/')
browser.get('http://python.org/')
browser.back()
time.sleep(1)
browser.forward()
browser.close()
```

### Cookies

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.zhihu.com/explore')
print(browser.get_cookies())
browser.add_cookie({'name': 'name', 'domain': 'www.zhihu.com', 'value': 'germey'})
print(browser.get_cookies())
browser.delete_all_cookies()
print(browser.get_cookies())
```
运行结果：
```
[{'domain': '.zhihu.com', 'httpOnly': False, 'name': 'Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49', 'path': '/', 'secure': False, 'value': '1574632295'}, {'domain': '.zhihu.com', 'expiry': 1606168295, 'httpOnly': False, 'name': 'Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49', 'path': '/', 'secure': False, 'value': '1574632295'}, {'domain': 'www.zhihu.com', 'expiry': 1574633193.39274, 'httpOnly': False, 'name': 'tgw_l7_route', 'path': '/', 'secure': False, 'value': '060f637cd101836814f6c53316f73463'}, {'domain': '.zhihu.com', 'expiry': 1669240293.393064, 'httpOnly': False, 'name': 'd_c0', 'path': '/', 'secure': False, 'value': '"AKAvpC3tZxCPTpjxr8sWHBQwuVqsYUAIHxY=|1574632294"'}, {'domain': '.zhihu.com', 'httpOnly': False, 'name': '_xsrf', 'path': '/', 'secure': False, 'value': 'a08e97a6-1b4c-4a42-80a4-f157b12f0e40'}, {'domain': '.zhihu.com', 'expiry': 1637704293.392961, 'httpOnly': False, 'name': '_zap', 'path': '/', 'secure': False, 'value': 'a23bc58f-40a5-41ce-b0c1-06556a1fb95e'}]
[{'domain': '.www.zhihu.com', 'httpOnly': False, 'name': 'name', 'path': '/', 'secure': True, 'value': 'germey'}, {'domain': '.zhihu.com', 'httpOnly': False, 'name': 'Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49', 'path': '/', 'secure': False, 'value': '1574632295'}, {'domain': '.zhihu.com', 'expiry': 1606168295, 'httpOnly': False, 'name': 'Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49', 'path': '/', 'secure': False, 'value': '1574632295'}, {'domain': 'www.zhihu.com', 'expiry': 1574633193.39274, 'httpOnly': False, 'name': 'tgw_l7_route', 'path': '/', 'secure': False, 'value': '060f637cd101836814f6c53316f73463'}, {'domain': '.zhihu.com', 'expiry': 1669240293.393064, 'httpOnly': False, 'name': 'd_c0', 'path': '/', 'secure': False, 'value': '"AKAvpC3tZxCPTpjxr8sWHBQwuVqsYUAIHxY=|1574632294"'}, {'domain': '.zhihu.com', 'httpOnly': False, 'name': '_xsrf', 'path': '/', 'secure': False, 'value': 'a08e97a6-1b4c-4a42-80a4-f157b12f0e40'}, {'domain': '.zhihu.com', 'expiry': 1637704293.392961, 'httpOnly': False, 'name': '_zap', 'path': '/', 'secure': False, 'value': 'a23bc58f-40a5-41ce-b0c1-06556a1fb95e'}]
[]
```

### 选项卡管理

```python
import time

from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.baidu.com/')
browser.execute_script('window.open()')
print(browser.window_handles)
browser.switch_to.window(browser.window_handles[1])
browser.get('http://www.taobao.com/')
time.sleep(1)
browser.switch_to.window(browser.window_handles[0])
browser.get('http://python.org')
```

### 异常处理

节点未找到异常
```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.baidu.com/')
browser.find_element_by_id('hello')
```
运行结果：
```
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="hello"]"}
  (Session info: chrome=78.0.3904.108)
```
捕获异常
```python
from selenium import webdriver
from selenium.common.exceptions import TimeoutException,NoSuchElementException

browser = webdriver.Chrome()
try:
    browser.get('http://www.baidu.com/')
except TimeoutException:
    print('Time OUT!')
try:
    browser.find_element_by_id('hello')
except NoSuchElementException:
    print('No Element')
finally:
    browser.close()
```

## 使用 Selenium 爬取淘宝商品
### 接口分析

打开淘宝首页，搜索你要搜索的商品名称，比如我这里搜索ipad,注意观察此时的url有什么变化(附上链接 https://s.taobao.com/search?q=ipad)，仔细观察便可以看到不同，然后查看网页源代码，找到商品列表所在的位置，如图所示，商品列表藏在m-itemlist 里面的items里面的item：

继续下拉，查询商品的详细信息，比如商品的图片，名称，价格，购买人数，店铺名称等，如图所示：

大概就是类似这样的信息。而我们要做的就是将这些信息抓取下来，再往下拉就可以发现，在页面下方，有一个分页导航，同时还有一个可以输入任意页码跳转的链接

这里商品的搜索结果一般都大于100页，要获取每一页的内容，只需要将页码从1到100遍历即可。所以，直接在页面跳转链接中输入要跳转的链接即可跳转到页码相应的页面。

### 获取商品列表

首先，需要构造一个抓取的URL：https://s.taobao.com/search?q=iPad。这个URL非常简洁，参数q就是要搜索的关键字。只要改变这个参数，即可获取不同商品的列表。这里我们将商品的关键字定义成一个变量，然后构造出这样的一个URL。

然后，就需要用Selenium进行抓取了。我们实现如下抓取列表页的方法：
```python
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from urllib.parse import quote

browser = webdriver.Chrome()
wait = WebDriverWait(browser, 10)
KEYWORD = 'iPad'

def index_page(page):
    """
    抓取索引页
    :param page: 页码
    """
    print('正在爬取第', page, '页')
    try:
        url = 'https://s.taobao.com/search?q=' + quote(KEYWORD)
        browser.get(url)
        if page > 1:
            input = wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-pager div.form > input')))
            submit = wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, '#mainsrp-pager div.form > span.btn.J_Submit')))
            input.clear()
            input.send_keys(page)
            submit.click()
        wait.until(
            EC.text_to_be_present_in_element((By.CSS_SELECTOR, '#mainsrp-pager li.item.active > span'), str(page)))
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.m-itemlist .items .item')))
        get_products()
    except TimeoutException:
        index_page(page)
```
这里首先构造了一个WebDriver对象，使用的浏览器是Chrome，然后指定一个关键词，如iPad，接着定义了index_page()方法，用于抓取商品列表页。

在该方法里，我们首先访问了搜索商品的链接，然后判断了当前的页码，如果大于1，就进行跳页操作，否则等待页面加载完成。

等待加载时，我们使用了WebDriverWait对象，它可以指定等待条件，同时指定一个最长等待时间，这里指定为最长10秒。如果在这个时间内成功匹配了等待条件，也就是说页面元素成功加载出来了，就立即返回相应结果并继续向下执行，否则到了最大等待时间还没有加载出来时，就直接抛出超时异常。

比如，我们最终要等待商品信息加载出来，就指定了presence_of_element_located这个条件，然后传入了.m-itemlist .items .item这个选择器，而这个选择器对应的页面内容就是每个商品的信息块，可以到网页里面查看一下。如果加载成功，就会执行后续的get_products()方法，提取商品信息。

关于翻页操作，这里首先获取页码输入框，赋值为input，然后获取“确定”按钮，赋值为submit

首先，我们清空了输入框，此时调用clear()方法即可。随后，调用send_keys()方法将页码填充到输入框中，然后点击“确定”按钮即可。

那么，怎样知道有没有跳转到对应的页码呢？我们可以注意到，成功跳转某一页后，页码都会高亮显示

我们只需要判断当前高亮的页码数是当前的页码数即可，所以这里使用了另一个等待条件text_to_be_present_in_element，它会等待指定的文本出现在某一个节点里面时即返回成功。这里我们将高亮的页码节点对应的CSS选择器和当前要跳转的页码通过参数传递给这个等待条件，这样它就会检测当前高亮的页码节点是不是我们传过来的页码数，如果是，就证明页面成功跳转到了这一页，页面跳转成功。

这样刚才实现的index_page()方法就可以传入对应的页码，待加载出对应页码的商品列表后，再去调用get_products()方法进行页面解析。

### 解析商品列表

接下来，我们就可以实现get_products()方法来解析商品列表了。这里我们直接获取页面源代码，然后用pyquery进行解析
```python
from pyquery import PyQuery as pq
def get_products():
    """
    提取商品数据
    """
    html = browser.page_source
    doc = pq(html)
    items = doc('#mainsrp-itemlist .items .item').items()
    for item in items:
        product = {
            'image': item.find('.pic .img').attr('data-src'),
            'price': item.find('.price').text(),
            'deal': item.find('.deal-cnt').text(),
            'title': item.find('.title').text(),
            'shop': item.find('.shop').text(),
            'location': item.find('.location').text()
        }
        print(product)
        save_to_mongo(product)
```
首先，调用page_source属性获取页码的源代码，然后构造了PyQuery解析对象，接着提取了商品列表，此时使用的CSS选择器是#mainsrp-itemlist .items .item，它会匹配整个页面的每个商品。它的匹配结果是多个，所以这里我们又对它进行了一次遍历，用for循环将每个结果分别进行解析，每次循环把它赋值为item变量，每个item变量都是一个PyQuery对象，然后再调用它的find()方法，传入CSS选择器，就可以获取单个商品的特定内容了。

因此，我们需要先利用find()方法找到图片的这个节点，然后再调用attr()方法获取商品的data-src属性，这样就成功提取了商品图片链接。然后用同样的方法提取商品的价格、成交量、名称、店铺和店铺所在地等信息，接着将所有提取结果赋值为一个字典product，随后调用save_to_mongo()将其保存到MongoDB即可。

### 保存到 MongoDB

```python
MONGO_URL = 'localhost'
MONGO_DB = 'taobao'
MONGO_COLLECTION = 'products'
client = pymongo.MongoClient(MONGO_URL)
db = client[MONGO_DB]
def save_to_mongo(result):
    """
    保存至MongoDB
    :param result: 结果
    """
    try:
        if db[MONGO_COLLECTION].insert(result):
            print('存储到MongoDB成功')
    except Exception:
        print('存储到MongoDB失败')
```

这里首先创建了一个MongoDB的连接对象，然后指定了数据库，随后指定了Collection的名称，接着直接调用insert()方法将数据插入到MongoDB。此处的result变量就是在get_products()方法里传来的product，包含单个商品的信息

### 遍历每页

刚才我们所定义的get_index()方法需要接收参数page，page代表页码。这里我们实现页码遍历即可
```python
MAX_PAGE = 100
def main():
    """
    遍历每一页
    """
    for i in range(1, MAX_PAGE + 1):
        index_page(i)
```

其实现非常简单，只需要调用一个for循环即可。这里定义最大的页码数为100，range()方法的返回结果就是1到100的列表，顺序遍历，调用index_page()方法即可。

### 运行

运行代码，可以发现首先会弹出一个Chrome浏览器，然后会访问淘宝页面，接着控制台便会输出相应的提取结果

可以发现，这些商品信息的结果都是字典形式，它们被存储到MongoDB里面。

### Chrome Headless 模式

从Chrome 59版本开始，已经开始支持Headless模式，也就是无界面模式，这样爬取的时候就不会弹出浏览器了。如果要使用此模式，请把Chrome升级到59版本及以上。启用Headless模式的方式如下：
```python
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
browser = webdriver.Chrome(chrome_options=chrome_options)
```
首先，创建ChromeOptions对象，接着添加headless参数，然后在初始化Chrome对象的时候通过chrome_options传递这个ChromeOptions对象，这样我们就可以成功启用Chrome的Headless模式了。

### 对接 Firefox

要对接Firefox浏览器，非常简单，只需要更改一处即可：
```python
browser = webdriver.Firefox()
```
这里更改了browser对象的创建方式，这样爬取的时候就会使用Firefox浏览器了。

### 对接 PhantomJS

如果不想使用Chrome的Headless模式，还可以使用PhantomJS（它是一个无界面浏览器）来抓取。抓取时，同样不会弹出窗口，还是只需要将WebDriver的声明修改一下即可：
```python
browser = webdriver.PhantomJS()
```
另外，它还支持命令行配置。比如，可以设置缓存和禁用图片加载的功能，进一步提高爬取效率：
```python
SERVICE_ARGS = ['--load-images=false', '--disk-cache=true']
browser = webdriver.PhantomJS(service_args=SERVICE_ARGS)
```
=======
---
layout: post
title: Selenium 的使用
subtitle: 
author: Balbo Cheng
categories: python
tags: [reptile]
---
## Selenium 的使用
### 基本使用

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait

browser = webdriver.Chrome()
try:
    browser.get('http://www.baidu.com')
    input = browser.find_element_by_id('kw')
    input.send_keys('Python')
    input.send_keys(Keys.ENTER)
    wait = WebDriverWait(browser, 10)
    wait.until(EC.presence_of_element_located((By.ID, 'content_left')))
    print(browser.current_url)
    print(browser.get_cookies())
    print(browser.page_source)
finally:
    browser.close()
```
如果用 Selenium 来驱动浏览器加载网页的话，就可以直接拿到 JavaScript 渲染的结果

### 声明浏览器对象

Selenium 支持非常多的浏览器，如 Chrom、Firefox、Edge等，还有 Android、BalckBerry等手机端的浏览器，还有无界面浏览器 PhantomJS

此外，我们可以用如下方式初始化
```python
from selenium import webdriver

browser = webdriver.Chrome()
browser = webdriver.Firefox()
browser = webdriver.Edge()
browser = webdriver.PhantomJS()
browser = webdriver.Safari()
```

### 访问页面

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://taobao.com/')
print(browser.page_source)
browser.close()
```

### 查找结点

- 单个节点

    想要从淘宝首页中提取搜索框这个节点，首先观察他的源代码

    代码实现
```python
    from selenium import webdriver
    
    browser = webdriver.Chrome()
browser.get('http://taobao.com/')
    input_first = browser.find_element_by_id('q')
    input_second = browser.find_element_by_css_selector('#q')
    input_third = browser.find_element_by_xpath('//*[@id="q"]')
    print(input_first, input_second, input_third)
    browser.close()
```
    这里使用3种方式获取输入框，他们返回的结果都一样都是：
    ```
    <selenium.webdriver.remote.webelement.WebElement (session="dd3692957c85930ca38ce1665fa2787f", element="262fbd50-eadb-4b7d-9a25-776e2e54020b")>
    ```
    另外，获取单个节点的方法还可以用
    ```python
    find_element_by_id()
    find_element_by_name()
    find_element_by_xpath()
    find_element_by_link_text()
    find_element_by_partial_link_text()
    find_element_by_tag_name()
    find_element_by_class_name()
    find_element_by_css_selector()
    
    # 通用方法
# 传入两个参数：查找方式和值
    find_element(By.xx, value)
    ```

- 多个节点

    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    browser.get('http://taobao.com/')
    lis = browser.find_elements_by_css_selector('.service-bd li')
    print(lis)
    browser.close()
    ```
    运行结果：
    ```python
    [<selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="5f8d80a7-847a-4213-a46b-1ece5e469075")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="d2bd82d2-ce90-489d-a827-eb9f13c81a05")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="3a27519d-ea9a-4d00-981d-a02bed240586")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="2ad04b30-84a0-43a9-bd48-455194a9ab6c")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="9ab80873-f1e9-4095-9b7b-5cfd63a4bc1c")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="4e8135ce-d1f7-4f16-a211-58307a96703b")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="4c7f9990-88a2-4731-b1cb-2b0e3b423447")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="c6bd9291-e744-4b95-85c0-ff2e9e37d026")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="0651f495-b40c-4236-9553-3ec9fc3cb2ff")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="87e5aaf1-7ace-45aa-ab33-fa283dde15f9")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="4aa4088e-8afc-43c9-8d96-0c367b0f4e32")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="7eb9b09d-cf45-4554-8c86-4c44fff8b089")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="10634f40-9a59-419c-b09f-6eb0817dd213")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="0efb2ecd-f2b1-478c-93da-c22f8cebef9a")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="525855b9-786b-4dd8-86c0-1dc8811d89f0")>, <selenium.webdriver.remote.webelement.WebElement (session="e933a9f34ca134090e701423f47f241c", element="b6dfe37c-730a-4ee1-a436-04b51d26cb1b")>]
    ```

### 节点交互

```python
import time

from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://taobao.com/')
input = browser.find_element_by_id('q')
input.send_keys('iPhone')
time.sleep(1)
input.clear()
input.send_keys('iPad')
button = browser.find_element_by_class_name('btn-search')
button.click()
```
通过上面的方法，我们完成了一些常见的动作操作，更多的操作可以参见官方文档的交互动作介绍： http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.remote.webelement

### 动作链

```python
from selenium import webdriver
from selenium.webdriver import ActionChains

browser = webdriver.Chrome()
url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'
browser.get(url)
browser.switch_to.frame('iframeResult')
source = browser.find_element_by_css_selector('#draggable')
target = browser.find_element_by_css_selector('#droppable')
actions = ActionChains(browser)
actions.drag_and_drop(source, target)
actions.perform()
```
更多的动作链操作可以参见官方文档： http://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains

### 执行 JavaScript

对于默写操作，Selenium API 没有提供，比如，下拉进度条
```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.zhihu.com/explore')
browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')
browser.execute_script('alert("To Bottom")')
```

### 获取节点信息

- 获取属性

    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    url = 'http://www.zhihu.com/explore'
    browser.get(url)
    logo = browser.find_element_by_id('zh-top-link-logo')
    print(logo)
    print(logo.get_attribute('class'))
    ```

    运行之后，程序便会驱动浏览器打开知乎页面，然后获取知乎的logo节点，最后打印出它的class
    ```
    <selenium.webdriver.remote.webelement.WebElement (session="e08c0f28d7f44d75ccd50df6bb676104", element="0.7236390660048155-1")>
    zu-top-link-logo
    ```

- 获取文本值

    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    url = 'http://www.zhihu.com/explore'
    browser.get(url)
    input = browser.find_element_by_class_name('zu-top-add-question')
    print(input.text)
    ```
    运行结果：
    ```
    提问
    ```


- 获取id、位置、标签名和大小

    WebElement节点还有一些其他属性，比如id属性可以获取节点id，location属性可以获取该节点在页面中的相对位置，tag_name属性可以获取标签名称，size属性可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的
    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    url = 'http://www.zhihu.com/explore'
    browser.get(url)
    input = browser.find_element_by_class_name('zu-top-add-question')
    print(input.id)
    print(input.location)
    print(input.tag_name)
    print(input.size)
    ```

### 切换 Frame

网页中有一种节点叫作iframe，也就是子Frame，相当于页面的子页面，它的结构和外部网页的结构完全一致。elenium打开页面后，它默认是在父级Frame里面操作，而此时如果页面中还有子Frame，它是不能获取到子Frame里面的节点的。这时就需要使用switch_to.frame()方法来切换Frame。
```python
from selenium import webdriver
from selenium.common.exceptions import NoSuchElementException

browser = webdriver.Chrome()
url = 'http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable'
browser.get(url)
browser.switch_to.frame('iframeResult')
try:
    logo = browser.find_element_by_class_name('logo')
except NoSuchElementException:
    print('No LOGO!')
browser.switch_to.parent_frame()
logo = browser.find_element_by_class_name('logo')
print(logo)
print(logo.text)
```
运行结果：
```
No LOGO!
<selenium.webdriver.remote.webelement.WebElement (session="a79db882fea6cf339ffff2980d14f4ed", element="8cf2debf-78f1-49d5-a689-4a31ae7306de")>
RUNOOB.COM
```
这里首先通过switch_to.frame()方法切换到子Frame里面，然后尝试获取父级Frame里的logo节点（这是不能找到的），如果找不到的话，就会抛出NoSuchElementException异常，异常被捕捉之后，就会输出NO LOGO。接下来，重新切换回父级Frame，然后再次重新获取节点，发现此时可以成功获取了。

所以，当页面中包含子Frame时，如果想获取子Frame中的节点，需要先调用switch_to.frame()方法切换到对应的Frame，然后再进行操作。

### 延时等待

在Selenium中，get()方法会在网页框架加载结束后结束执行，此时如果获取page_source，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。所以，这里需要延时等待一定时间，确保节点已经加载出来。

- 隐式等待

    ```python
    from selenium import webdriver

    browser = webdriver.Chrome()
    browser.implicitly_wait(10)
    browser.get('http://www.zhihu.com/explore')
    input = browser.find_element_by_class_name('zu-top-ass-question')
    print(input)
    ```

- 显示等待

    ```python
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC

    browser = webdriver.Chrome()
    browser.get('http://www.taobao.com/')
    wait = WebDriverWait(browser, 10)
    input = wait.until(EC.presence_of_element_located((By.ID, 'q')))
    button = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, '.btn-search')))
    print(input, button)
    ```

| 等待条件 | 含义 |
| ---- | ---- |
| title_is | 标题是某内容 |
| title_contains | 标题包含某内容 |
| presence_of_element_located | 节点加载出来，传入定位元组 |
| visibilty_of_element_located | 节点可见，传入定位元组 |
| visibilty_of | 可见，传入节点对象 |
| presence_of_all_elements_loacted | 所有节点加载出来 |
| text_to_be_present_in_element | 某个节点文本包含某文字 |
| text_to_be_present_in_element_value | 某个节点值包含某文字 |
| frame_to_be_available_and_switch_to_it | 加载并切换 |
| invisibility_of_element_located | 节点不可见 |
| element_to_be_clickable | 节点可点击 |
| staleness_of | 判断一个节点是否仍在 DOM，可判断页面是否已经刷新 |
| element_to_be_selected | 节点可选择，传节点对象 |
| element_located_to_be_selected | 节点可选择，传入定位元组 |
| element_selection_state_to_be | 传入节点对象以及状态，相等返回 True，否则返回 False |
| element_located_selection_state_to_be | 传入定位元组以及状态，相等返回 True，否则返回 False |
| alert_is_present | 是否出现警告 |

### 前进和后退

```python
import time

from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.baidu.com/')
browser.get('http://www.taobao.com/')
browser.get('http://python.org/')
browser.back()
time.sleep(1)
browser.forward()
browser.close()
```

### Cookies

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.zhihu.com/explore')
print(browser.get_cookies())
browser.add_cookie({'name': 'name', 'domain': 'www.zhihu.com', 'value': 'germey'})
print(browser.get_cookies())
browser.delete_all_cookies()
print(browser.get_cookies())
```
运行结果：
```
[{'domain': '.zhihu.com', 'httpOnly': False, 'name': 'Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49', 'path': '/', 'secure': False, 'value': '1574632295'}, {'domain': '.zhihu.com', 'expiry': 1606168295, 'httpOnly': False, 'name': 'Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49', 'path': '/', 'secure': False, 'value': '1574632295'}, {'domain': 'www.zhihu.com', 'expiry': 1574633193.39274, 'httpOnly': False, 'name': 'tgw_l7_route', 'path': '/', 'secure': False, 'value': '060f637cd101836814f6c53316f73463'}, {'domain': '.zhihu.com', 'expiry': 1669240293.393064, 'httpOnly': False, 'name': 'd_c0', 'path': '/', 'secure': False, 'value': '"AKAvpC3tZxCPTpjxr8sWHBQwuVqsYUAIHxY=|1574632294"'}, {'domain': '.zhihu.com', 'httpOnly': False, 'name': '_xsrf', 'path': '/', 'secure': False, 'value': 'a08e97a6-1b4c-4a42-80a4-f157b12f0e40'}, {'domain': '.zhihu.com', 'expiry': 1637704293.392961, 'httpOnly': False, 'name': '_zap', 'path': '/', 'secure': False, 'value': 'a23bc58f-40a5-41ce-b0c1-06556a1fb95e'}]
[{'domain': '.www.zhihu.com', 'httpOnly': False, 'name': 'name', 'path': '/', 'secure': True, 'value': 'germey'}, {'domain': '.zhihu.com', 'httpOnly': False, 'name': 'Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49', 'path': '/', 'secure': False, 'value': '1574632295'}, {'domain': '.zhihu.com', 'expiry': 1606168295, 'httpOnly': False, 'name': 'Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49', 'path': '/', 'secure': False, 'value': '1574632295'}, {'domain': 'www.zhihu.com', 'expiry': 1574633193.39274, 'httpOnly': False, 'name': 'tgw_l7_route', 'path': '/', 'secure': False, 'value': '060f637cd101836814f6c53316f73463'}, {'domain': '.zhihu.com', 'expiry': 1669240293.393064, 'httpOnly': False, 'name': 'd_c0', 'path': '/', 'secure': False, 'value': '"AKAvpC3tZxCPTpjxr8sWHBQwuVqsYUAIHxY=|1574632294"'}, {'domain': '.zhihu.com', 'httpOnly': False, 'name': '_xsrf', 'path': '/', 'secure': False, 'value': 'a08e97a6-1b4c-4a42-80a4-f157b12f0e40'}, {'domain': '.zhihu.com', 'expiry': 1637704293.392961, 'httpOnly': False, 'name': '_zap', 'path': '/', 'secure': False, 'value': 'a23bc58f-40a5-41ce-b0c1-06556a1fb95e'}]
[]
```

### 选项卡管理

```python
import time

from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.baidu.com/')
browser.execute_script('window.open()')
print(browser.window_handles)
browser.switch_to.window(browser.window_handles[1])
browser.get('http://www.taobao.com/')
time.sleep(1)
browser.switch_to.window(browser.window_handles[0])
browser.get('http://python.org')
```

### 异常处理

节点未找到异常
```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get('http://www.baidu.com/')
browser.find_element_by_id('hello')
```
运行结果：
```
selenium.common.exceptions.NoSuchElementException: Message: no such element: Unable to locate element: {"method":"css selector","selector":"[id="hello"]"}
  (Session info: chrome=78.0.3904.108)
```
捕获异常
```python
from selenium import webdriver
from selenium.common.exceptions import TimeoutException,NoSuchElementException

browser = webdriver.Chrome()
try:
    browser.get('http://www.baidu.com/')
except TimeoutException:
    print('Time OUT!')
try:
    browser.find_element_by_id('hello')
except NoSuchElementException:
    print('No Element')
finally:
    browser.close()
```

## 使用 Selenium 爬取淘宝商品
### 接口分析

打开淘宝首页，搜索你要搜索的商品名称，比如我这里搜索ipad,注意观察此时的url有什么变化(附上链接 https://s.taobao.com/search?q=ipad)，仔细观察便可以看到不同，然后查看网页源代码，找到商品列表所在的位置，如图所示，商品列表藏在m-itemlist 里面的items里面的item：

继续下拉，查询商品的详细信息，比如商品的图片，名称，价格，购买人数，店铺名称等，如图所示：

大概就是类似这样的信息。而我们要做的就是将这些信息抓取下来，再往下拉就可以发现，在页面下方，有一个分页导航，同时还有一个可以输入任意页码跳转的链接

这里商品的搜索结果一般都大于100页，要获取每一页的内容，只需要将页码从1到100遍历即可。所以，直接在页面跳转链接中输入要跳转的链接即可跳转到页码相应的页面。

### 获取商品列表

首先，需要构造一个抓取的URL：https://s.taobao.com/search?q=iPad。这个URL非常简洁，参数q就是要搜索的关键字。只要改变这个参数，即可获取不同商品的列表。这里我们将商品的关键字定义成一个变量，然后构造出这样的一个URL。

然后，就需要用Selenium进行抓取了。我们实现如下抓取列表页的方法：
```python
from selenium import webdriver
from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.wait import WebDriverWait
from urllib.parse import quote

browser = webdriver.Chrome()
wait = WebDriverWait(browser, 10)
KEYWORD = 'iPad'

def index_page(page):
    """
    抓取索引页
    :param page: 页码
    """
    print('正在爬取第', page, '页')
    try:
        url = 'https://s.taobao.com/search?q=' + quote(KEYWORD)
        browser.get(url)
        if page > 1:
            input = wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, '#mainsrp-pager div.form > input')))
            submit = wait.until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, '#mainsrp-pager div.form > span.btn.J_Submit')))
            input.clear()
            input.send_keys(page)
            submit.click()
        wait.until(
            EC.text_to_be_present_in_element((By.CSS_SELECTOR, '#mainsrp-pager li.item.active > span'), str(page)))
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, '.m-itemlist .items .item')))
        get_products()
    except TimeoutException:
        index_page(page)
```
这里首先构造了一个WebDriver对象，使用的浏览器是Chrome，然后指定一个关键词，如iPad，接着定义了index_page()方法，用于抓取商品列表页。

在该方法里，我们首先访问了搜索商品的链接，然后判断了当前的页码，如果大于1，就进行跳页操作，否则等待页面加载完成。

等待加载时，我们使用了WebDriverWait对象，它可以指定等待条件，同时指定一个最长等待时间，这里指定为最长10秒。如果在这个时间内成功匹配了等待条件，也就是说页面元素成功加载出来了，就立即返回相应结果并继续向下执行，否则到了最大等待时间还没有加载出来时，就直接抛出超时异常。

比如，我们最终要等待商品信息加载出来，就指定了presence_of_element_located这个条件，然后传入了.m-itemlist .items .item这个选择器，而这个选择器对应的页面内容就是每个商品的信息块，可以到网页里面查看一下。如果加载成功，就会执行后续的get_products()方法，提取商品信息。

关于翻页操作，这里首先获取页码输入框，赋值为input，然后获取“确定”按钮，赋值为submit

首先，我们清空了输入框，此时调用clear()方法即可。随后，调用send_keys()方法将页码填充到输入框中，然后点击“确定”按钮即可。

那么，怎样知道有没有跳转到对应的页码呢？我们可以注意到，成功跳转某一页后，页码都会高亮显示

我们只需要判断当前高亮的页码数是当前的页码数即可，所以这里使用了另一个等待条件text_to_be_present_in_element，它会等待指定的文本出现在某一个节点里面时即返回成功。这里我们将高亮的页码节点对应的CSS选择器和当前要跳转的页码通过参数传递给这个等待条件，这样它就会检测当前高亮的页码节点是不是我们传过来的页码数，如果是，就证明页面成功跳转到了这一页，页面跳转成功。

这样刚才实现的index_page()方法就可以传入对应的页码，待加载出对应页码的商品列表后，再去调用get_products()方法进行页面解析。

### 解析商品列表

接下来，我们就可以实现get_products()方法来解析商品列表了。这里我们直接获取页面源代码，然后用pyquery进行解析
```python
from pyquery import PyQuery as pq
def get_products():
    """
    提取商品数据
    """
    html = browser.page_source
    doc = pq(html)
    items = doc('#mainsrp-itemlist .items .item').items()
    for item in items:
        product = {
            'image': item.find('.pic .img').attr('data-src'),
            'price': item.find('.price').text(),
            'deal': item.find('.deal-cnt').text(),
            'title': item.find('.title').text(),
            'shop': item.find('.shop').text(),
            'location': item.find('.location').text()
        }
        print(product)
        save_to_mongo(product)
```
首先，调用page_source属性获取页码的源代码，然后构造了PyQuery解析对象，接着提取了商品列表，此时使用的CSS选择器是#mainsrp-itemlist .items .item，它会匹配整个页面的每个商品。它的匹配结果是多个，所以这里我们又对它进行了一次遍历，用for循环将每个结果分别进行解析，每次循环把它赋值为item变量，每个item变量都是一个PyQuery对象，然后再调用它的find()方法，传入CSS选择器，就可以获取单个商品的特定内容了。

因此，我们需要先利用find()方法找到图片的这个节点，然后再调用attr()方法获取商品的data-src属性，这样就成功提取了商品图片链接。然后用同样的方法提取商品的价格、成交量、名称、店铺和店铺所在地等信息，接着将所有提取结果赋值为一个字典product，随后调用save_to_mongo()将其保存到MongoDB即可。

### 保存到 MongoDB

```python
MONGO_URL = 'localhost'
MONGO_DB = 'taobao'
MONGO_COLLECTION = 'products'
client = pymongo.MongoClient(MONGO_URL)
db = client[MONGO_DB]
def save_to_mongo(result):
    """
    保存至MongoDB
    :param result: 结果
    """
    try:
        if db[MONGO_COLLECTION].insert(result):
            print('存储到MongoDB成功')
    except Exception:
        print('存储到MongoDB失败')
```

这里首先创建了一个MongoDB的连接对象，然后指定了数据库，随后指定了Collection的名称，接着直接调用insert()方法将数据插入到MongoDB。此处的result变量就是在get_products()方法里传来的product，包含单个商品的信息

### 遍历每页

刚才我们所定义的get_index()方法需要接收参数page，page代表页码。这里我们实现页码遍历即可
```python
MAX_PAGE = 100
def main():
    """
    遍历每一页
    """
    for i in range(1, MAX_PAGE + 1):
        index_page(i)
```

其实现非常简单，只需要调用一个for循环即可。这里定义最大的页码数为100，range()方法的返回结果就是1到100的列表，顺序遍历，调用index_page()方法即可。

### 运行

运行代码，可以发现首先会弹出一个Chrome浏览器，然后会访问淘宝页面，接着控制台便会输出相应的提取结果

可以发现，这些商品信息的结果都是字典形式，它们被存储到MongoDB里面。

### Chrome Headless 模式

从Chrome 59版本开始，已经开始支持Headless模式，也就是无界面模式，这样爬取的时候就不会弹出浏览器了。如果要使用此模式，请把Chrome升级到59版本及以上。启用Headless模式的方式如下：
```python
chrome_options = webdriver.ChromeOptions()
chrome_options.add_argument('--headless')
browser = webdriver.Chrome(chrome_options=chrome_options)
```
首先，创建ChromeOptions对象，接着添加headless参数，然后在初始化Chrome对象的时候通过chrome_options传递这个ChromeOptions对象，这样我们就可以成功启用Chrome的Headless模式了。

### 对接 Firefox

要对接Firefox浏览器，非常简单，只需要更改一处即可：
```python
browser = webdriver.Firefox()
```
这里更改了browser对象的创建方式，这样爬取的时候就会使用Firefox浏览器了。

### 对接 PhantomJS

如果不想使用Chrome的Headless模式，还可以使用PhantomJS（它是一个无界面浏览器）来抓取。抓取时，同样不会弹出窗口，还是只需要将WebDriver的声明修改一下即可：
```python
browser = webdriver.PhantomJS()
```
另外，它还支持命令行配置。比如，可以设置缓存和禁用图片加载的功能，进一步提高爬取效率：
```python
SERVICE_ARGS = ['--load-images=false', '--disk-cache=true']
browser = webdriver.PhantomJS(service_args=SERVICE_ARGS)
```
>>>>>>> e5c77ca (新页面)
