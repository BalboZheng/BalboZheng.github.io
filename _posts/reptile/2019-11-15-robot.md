---
layout:     post
title:      "Robots 协议"
subtitle:   "分析robots协议"
date:       2019-11-15 19:50:25
author:     "Balbo"
header-img: "img/post-bg-2019.jpg"
catalog: true
tags:
    - reptile
---
利用 urllib 的 robotparser 模块，我们可以实现网站 Robots 协议的分析。

## Robots 协议
Robots 协议就是每个网站对于来到的爬虫所提出的要求。(并非强制要求遵守的协议，只是一种建议，但是如果不遵守有可能会承担法律责任。）

每个网站的 Robots 协议都在该网站的根目录下，例如百度的 Robots 协议的位置就是 https://www.baidu.com/robots.txt 或者京东的 Robots 协议就在 https://www.jd.com/robots.txt

下面给出一段京东的 Robots 的内容：
```
User-agent: * 
Disallow: /?* 
Disallow: /pop/*.html 
Disallow: /pinpai/*.html?* 
User-agent: EtaoSpider 
Disallow: / 
User-agent: HuihuiSpider 
Disallow: / 
User-agent: GwdangSpider 
Disallow: / 
User-agent: WochachaSpider 
Disallow: /
```

第一个的意思就是说对于所有的爬虫，不能爬取在/？开头的路径，也不能访问和/pop/*.html 匹配的路径。
后面四个用户代理的爬虫不允许访问任何资源。

所以 Robots 协议的基本语法如下：
```
User-agent: 这里是爬虫的名字
Disallow: /该爬虫不允许访问的内容
```

## robotparser
连接 Robots 协议之后，我们就可以使用 robotparser 模块来解析 robots.txt。该模块提供了一个类 RobotFileParser，他可以根据摸网站的 robots.txt 文件来频段一个爬取爬出是否有权限来爬取这个网站

```python
from urllib.robotparser import RobotFileParser

rp = RobotFileParser()
rp.set_url('http://www.jianshu.com/robots.txt')
rp.read()
print(rp.can_fetch('*', 'https://www.jianshu.com/p/46034a031b7a'))
print(rp.can_fetch('*', 'http://www.jianshu.com/search?q=python&page=1&type=collections'))
```
运行结果：
```
False
False
```
